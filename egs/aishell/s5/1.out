local/aishell_prepare_dict.sh: AISHELL dict preparation succeeded
-------------------------------------
Preparing data/local/train transcriptions
Preparing data/local/dev transcriptions
Preparing data/local/test transcriptions
local/aishell_data_prep.sh: AISHELL data preparation succeeded
-------------------------------------
utils/prepare_lang.sh --position-dependent-phones false data/local/dict <SPOKEN_NOISE> data/local/lang data/lang
Checking data/local/dict/silence_phones.txt ...
--> reading data/local/dict/silence_phones.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/silence_phones.txt is OK

Checking data/local/dict/optional_silence.txt ...
--> reading data/local/dict/optional_silence.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/optional_silence.txt is OK

Checking data/local/dict/nonsilence_phones.txt ...
--> reading data/local/dict/nonsilence_phones.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/nonsilence_phones.txt is OK

Checking disjoint: silence_phones.txt, nonsilence_phones.txt
--> disjoint property is OK.

Checking data/local/dict/lexicon.txt
--> reading data/local/dict/lexicon.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/lexicon.txt is OK

Checking data/local/dict/extra_questions.txt ...
--> reading data/local/dict/extra_questions.txt
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/local/dict/extra_questions.txt is OK
--> SUCCESS [validating dictionary directory data/local/dict]

**Creating data/local/dict/lexiconp.txt from data/local/dict/lexicon.txt
fstaddselfloops data/lang/phones/wdisambig_phones.int data/lang/phones/wdisambig_words.int 
prepare_lang.sh: validating output directory
utils/validate_lang.pl data/lang
Checking data/lang/phones.txt ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/lang/phones.txt is OK

Checking words.txt: #0 ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> data/lang/words.txt is OK

Checking disjoint: silence.txt, nonsilence.txt, disambig.txt ...
--> silence.txt and nonsilence.txt are disjoint
--> silence.txt and disambig.txt are disjoint
--> disambig.txt and nonsilence.txt are disjoint
--> disjoint property is OK

Checking sumation: silence.txt, nonsilence.txt, disambig.txt ...
--> found no unexplainable phones in phones.txt

Checking data/lang/phones/context_indep.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.int corresponds to data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.csl corresponds to data/lang/phones/context_indep.txt
--> data/lang/phones/context_indep.{txt, int, csl} are OK

Checking data/lang/phones/nonsilence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 216 entry/entries in data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.int corresponds to data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.csl corresponds to data/lang/phones/nonsilence.txt
--> data/lang/phones/nonsilence.{txt, int, csl} are OK

Checking data/lang/phones/silence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/silence.txt
--> data/lang/phones/silence.int corresponds to data/lang/phones/silence.txt
--> data/lang/phones/silence.csl corresponds to data/lang/phones/silence.txt
--> data/lang/phones/silence.{txt, int, csl} are OK

Checking data/lang/phones/optional_silence.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.int corresponds to data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.csl corresponds to data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.{txt, int, csl} are OK

Checking data/lang/phones/disambig.{txt, int, csl} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 105 entry/entries in data/lang/phones/disambig.txt
--> data/lang/phones/disambig.int corresponds to data/lang/phones/disambig.txt
--> data/lang/phones/disambig.csl corresponds to data/lang/phones/disambig.txt
--> data/lang/phones/disambig.{txt, int, csl} are OK

Checking data/lang/phones/roots.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 67 entry/entries in data/lang/phones/roots.txt
--> data/lang/phones/roots.int corresponds to data/lang/phones/roots.txt
--> data/lang/phones/roots.{txt, int} are OK

Checking data/lang/phones/sets.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 67 entry/entries in data/lang/phones/sets.txt
--> data/lang/phones/sets.int corresponds to data/lang/phones/sets.txt
--> data/lang/phones/sets.{txt, int} are OK

Checking data/lang/phones/extra_questions.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 7 entry/entries in data/lang/phones/extra_questions.txt
--> data/lang/phones/extra_questions.int corresponds to data/lang/phones/extra_questions.txt
--> data/lang/phones/extra_questions.{txt, int} are OK

Checking optional_silence.txt ...
--> reading data/lang/phones/optional_silence.txt
--> data/lang/phones/optional_silence.txt is OK

Checking disambiguation symbols: #0 and #1
--> data/lang/phones/disambig.txt has "#0" and "#1"
--> data/lang/phones/disambig.txt is OK

Checking topo ...

Checking word-level disambiguation symbols...
--> data/lang/phones/wdisambig.txt exists (newer prepare_lang.sh)
Checking data/lang/oov.{txt, int} ...
--> text seems to be UTF-8 or ASCII, checking whitespaces
--> text contains only allowed whitespaces
--> 1 entry/entries in data/lang/oov.txt
--> data/lang/oov.int corresponds to data/lang/oov.txt
--> data/lang/oov.{txt, int} are OK

--> data/lang/L.fst is olabel sorted
--> data/lang/L_disambig.fst is olabel sorted
--> SUCCESS [validating lang directory data/lang]
-------------------------------------
Getting raw N-gram counts
discount_ngrams: for n-gram order 1, D=0.000000, tau=0.000000 phi=1.000000
discount_ngrams: for n-gram order 2, D=0.000000, tau=0.000000 phi=1.000000
discount_ngrams: for n-gram order 3, D=1.000000, tau=0.000000 phi=1.000000
Iteration 1/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.675000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.675000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.825000 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.900000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.900000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.100000 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=1.215000 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=1.215000 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.485000 phi=2.000000
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 573.088187
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 573.088187
Perplexity over 99496.000000 words is 571.860357
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 571.860357

real	0m4.052s
user	0m5.254s
sys	0m0.185s

real	0m4.087s
user	0m5.314s
sys	0m0.150s
Perplexity over 99496.000000 words is 571.430399
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 571.430399

real	0m4.282s
user	0m5.576s
sys	0m0.136s
Projected perplexity change from setting alpha=-0.413521475380432 is 571.860357->571.350704659834, reduction of 0.509652340166213
Alpha value on iter 1 is -0.413521475380432
Iteration 2/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.483845 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.645126 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=0.870921 phi=2.000000
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 570.909914
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.909914

real	0m4.052s
user	0m5.300s
sys	0m0.147s
Perplexity over 99496.000000 words is 570.209333
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.209333
Perplexity over 99496.000000 words is 570.548231
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.548231

real	0m4.095s
user	0m5.326s
sys	0m0.145s

real	0m4.105s
user	0m5.353s
sys	0m0.179s
optimize_alpha.pl: alpha=0.782133003937562 is too positive, limiting it to 0.7
Projected perplexity change from setting alpha=0.7 is 570.548231->570.0658029, reduction of 0.482428099999765
Alpha value on iter 2 is 0.7
Iteration 3/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.750000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=2.000000
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=2.350000
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 570.074175
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.074175
Perplexity over 99496.000000 words is 570.070852
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.070852

real	0m4.054s
user	0m5.285s
sys	0m0.165s

real	0m4.081s
user	0m5.290s
sys	0m0.181s
Perplexity over 99496.000000 words is 570.135232
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.135232

real	0m4.308s
user	0m5.573s
sys	0m0.154s
Projected perplexity change from setting alpha=-0.149743638839048 is 570.074175->570.068152268062, reduction of 0.00602273193794645
Alpha value on iter 3 is -0.149743638839048
Iteration 4/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.800000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=1.080000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 651.559076
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 651.559076

real	0m2.796s
user	0m3.498s
sys	0m0.132s
Perplexity over 99496.000000 words is 571.811721
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 571.811721
Perplexity over 99496.000000 words is 570.079098
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 570.079098

real	0m4.090s
user	0m5.302s
sys	0m0.125s

real	0m4.095s
user	0m5.288s
sys	0m0.174s
Projected perplexity change from setting alpha=-0.116327143544381 is 570.079098->564.672375993263, reduction of 5.40672200673657
Alpha value on iter 4 is -0.116327143544381
Iteration 5/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.395873 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.712571 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 567.407206
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.407206

real	0m4.026s
user	0m5.259s
sys	0m0.164s
Perplexity over 99496.000000 words is 567.980179
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.980179
Perplexity over 99496.000000 words is 567.231151
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.231151

real	0m4.063s
user	0m5.289s
sys	0m0.179s

real	0m4.088s
user	0m5.334s
sys	0m0.165s
Projected perplexity change from setting alpha=0.259356959958262 is 567.407206->567.206654822021, reduction of 0.20055117797915
Alpha value on iter 5 is 0.259356959958262
Iteration 6/6 of optimizing discounting parameters
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.664727 phi=1.750000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.664727 phi=2.000000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.664727 phi=2.350000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 567.346876
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.346876

real	0m4.026s
user	0m5.231s
sys	0m0.175s
Perplexity over 99496.000000 words is 567.181130
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.181130

real	0m4.060s
user	0m5.304s
sys	0m0.148s
Perplexity over 99496.000000 words is 567.478625
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.478625

real	0m4.146s
user	0m5.388s
sys	0m0.147s
optimize_alpha.pl: alpha=2.83365708509299 is too positive, limiting it to 0.7
Projected perplexity change from setting alpha=0.7 is 567.346876->567.0372037, reduction of 0.309672299999761
Alpha value on iter 6 is 0.7
Final config is:
D=0.6 tau=0.527830672157611 phi=2
D=0.706938285164495 tau=0.664727230661135 phi=2.7
D=0 tau=1.09671484103859 phi=1.85025636116095
Discounting N-grams.
discount_ngrams: for n-gram order 1, D=0.600000, tau=0.527831 phi=2.000000
discount_ngrams: for n-gram order 2, D=0.706938, tau=0.664727 phi=2.700000
discount_ngrams: for n-gram order 3, D=0.000000, tau=1.096715 phi=1.850256
Computing final perplexity
Building ARPA LM (perplexity computation is in background)
interpolate_ngrams: 137074 words in wordslist
interpolate_ngrams: 137074 words in wordslist
Perplexity over 99496.000000 words is 567.320537
Perplexity over 99496.000000 words (excluding 0.000000 OOVs) is 567.320537
567.320537
Done training LM of type 3gram-mincount
-------------------------------------
Converting 'data/local/lm/3gram-mincount/lm_unpruned.gz' to FST
arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G.fst 
LOG (arpa2fst[5.5.266~1-77ac7]:Read():arpa-file-parser.cc:94) Reading \data\ section.
LOG (arpa2fst[5.5.266~1-77ac7]:Read():arpa-file-parser.cc:149) Reading \1-grams: section.
LOG (arpa2fst[5.5.266~1-77ac7]:Read():arpa-file-parser.cc:149) Reading \2-grams: section.
LOG (arpa2fst[5.5.266~1-77ac7]:Read():arpa-file-parser.cc:149) Reading \3-grams: section.
LOG (arpa2fst[5.5.266~1-77ac7]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 561655 to 102646
fstisstochastic data/lang_test/G.fst 
8.84583e-06 -0.56498
Succeeded in formatting LM: 'data/local/lm/3gram-mincount/lm_unpruned.gz'
-------------------------------------
steps/make_mfcc_pitch.sh --cmd queue.pl --mem 2G --nj 10 data/train exp/make_mfcc/train mfcc
utils/validate_data_dir.sh: Successfully validated data-directory data/train
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
queue.pl: Error submitting jobs to queue (return status was 32512)
queue log file is exp/make_mfcc/train/q/make_mfcc_pitch_train.log, command was qsub -v PATH -cwd -S /bin/bash -j y -l arch=*64* -o exp/make_mfcc/train/q/make_mfcc_pitch_train.log  -l mem_free=2G,ram_free=2G  -t 1:10 /home/data/yelong/kaldi/egs/aishell/s5/exp/make_mfcc/train/q/make_mfcc_pitch_train.sh >>exp/make_mfcc/train/q/make_mfcc_pitch_train.log 2>&1
Output of qsub was: sh: qsub: command not found

-------------------------------------
steps/make_mfcc_pitch.sh --cmd run.pl --nj 10 data/train exp/make_mfcc/train mfcc
utils/validate_data_dir.sh: Successfully validated data-directory data/train
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for train
steps/compute_cmvn_stats.sh data/train exp/make_mfcc/train mfcc
Succeeded creating CMVN stats for train
fix_data_dir.sh: kept all 120098 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
steps/make_mfcc_pitch.sh --cmd run.pl --nj 10 data/dev exp/make_mfcc/dev mfcc
utils/validate_data_dir.sh: Successfully validated data-directory data/dev
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for dev
steps/compute_cmvn_stats.sh data/dev exp/make_mfcc/dev mfcc
Succeeded creating CMVN stats for dev
fix_data_dir.sh: kept all 14326 utterances.
fix_data_dir.sh: old files are kept in data/dev/.backup
steps/make_mfcc_pitch.sh --cmd run.pl --nj 10 data/test exp/make_mfcc/test mfcc
utils/validate_data_dir.sh: Successfully validated data-directory data/test
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for test
steps/compute_cmvn_stats.sh data/test exp/make_mfcc/test mfcc
Succeeded creating CMVN stats for test
fix_data_dir.sh: kept all 7176 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
-------------------------------------
steps/make_mfcc_pitch.sh --cmd run.pl --nj 32 data/train exp/make_mfcc/train mfcc
steps/make_mfcc_pitch.sh: moving data/train/feats.scp to data/train/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/train
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for train
steps/compute_cmvn_stats.sh data/train exp/make_mfcc/train mfcc
Succeeded creating CMVN stats for train
fix_data_dir.sh: kept all 120098 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
steps/make_mfcc_pitch.sh --cmd run.pl --nj 32 data/dev exp/make_mfcc/dev mfcc
steps/make_mfcc_pitch.sh: moving data/dev/feats.scp to data/dev/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/dev
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for dev
steps/compute_cmvn_stats.sh data/dev exp/make_mfcc/dev mfcc
Succeeded creating CMVN stats for dev
fix_data_dir.sh: kept all 14326 utterances.
fix_data_dir.sh: old files are kept in data/dev/.backup
steps/make_mfcc_pitch.sh --cmd run.pl --nj 32 data/test exp/make_mfcc/test mfcc
steps/make_mfcc_pitch.sh: moving data/test/feats.scp to data/test/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for test
steps/compute_cmvn_stats.sh data/test exp/make_mfcc/test mfcc
Succeeded creating CMVN stats for test
fix_data_dir.sh: kept all 7176 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
steps/train_mono.sh --cmd run.pl --nj 32 data/train data/lang exp/mono
steps/train_mono.sh: Initializing monophone system.
steps/train_mono.sh: Compiling training graphs
steps/train_mono.sh: Aligning data equally (pass 0)
steps/train_mono.sh: Pass 1
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 2
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 3
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 4
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 5
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 6
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 7
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 8
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 9
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 10
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 11
steps/train_mono.sh: Pass 12
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 13
steps/train_mono.sh: Pass 14
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 15
steps/train_mono.sh: Pass 16
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 17
steps/train_mono.sh: Pass 18
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 19
steps/train_mono.sh: Pass 20
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 21
steps/train_mono.sh: Pass 22
steps/train_mono.sh: Pass 23
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 24
steps/train_mono.sh: Pass 25
steps/train_mono.sh: Pass 26
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 27
steps/train_mono.sh: Pass 28
steps/train_mono.sh: Pass 29
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 30
steps/train_mono.sh: Pass 31
steps/train_mono.sh: Pass 32
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 33
steps/train_mono.sh: Pass 34
steps/train_mono.sh: Pass 35
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 36
steps/train_mono.sh: Pass 37
steps/train_mono.sh: Pass 38
steps/train_mono.sh: Aligning data
steps/train_mono.sh: Pass 39
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/mono
steps/diagnostic/analyze_alignments.sh: see stats in exp/mono/log/analyze_alignments.log
1192 warnings in exp/mono/log/acc.*.*.log
38347 warnings in exp/mono/log/align.*.*.log
exp/mono: nj=32 align prob=-82.05 over 150.15h [retry=1.0%, fail=0.0%] states=203 gauss=986
steps/train_mono.sh: Done training monophone system in exp/mono
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/mono exp/mono_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/mono, putting alignments in exp/mono_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/mono_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/mono_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
steps/train_deltas.sh --cmd run.pl 2500 20000 data/train data/lang exp/mono_ali exp/tri1
steps/train_deltas.sh: accumulating tree stats
steps/train_deltas.sh: getting questions for tree-building, via clustering
steps/train_deltas.sh: building the tree
steps/train_deltas.sh: converting alignments from exp/mono_ali to use current tree
steps/train_deltas.sh: compiling graphs of transcripts
steps/train_deltas.sh: training pass 1
steps/train_deltas.sh: training pass 2
steps/train_deltas.sh: training pass 3
steps/train_deltas.sh: training pass 4
steps/train_deltas.sh: training pass 5
steps/train_deltas.sh: training pass 6
steps/train_deltas.sh: training pass 7
steps/train_deltas.sh: training pass 8
steps/train_deltas.sh: training pass 9
steps/train_deltas.sh: training pass 10
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 11
steps/train_deltas.sh: training pass 12
steps/train_deltas.sh: training pass 13
steps/train_deltas.sh: training pass 14
steps/train_deltas.sh: training pass 15
steps/train_deltas.sh: training pass 16
steps/train_deltas.sh: training pass 17
steps/train_deltas.sh: training pass 18
steps/train_deltas.sh: training pass 19
steps/train_deltas.sh: training pass 20
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 21
steps/train_deltas.sh: training pass 22
steps/train_deltas.sh: training pass 23
steps/train_deltas.sh: training pass 24
steps/train_deltas.sh: training pass 25
steps/train_deltas.sh: training pass 26
steps/train_deltas.sh: training pass 27
steps/train_deltas.sh: training pass 28
steps/train_deltas.sh: training pass 29
steps/train_deltas.sh: training pass 30
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 31
steps/train_deltas.sh: training pass 32
steps/train_deltas.sh: training pass 33
steps/train_deltas.sh: training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri1
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1/log/analyze_alignments.log
1 warnings in exp/tri1/log/compile_questions.log
981 warnings in exp/tri1/log/acc.*.*.log
3365 warnings in exp/tri1/log/align.*.*.log
1 warnings in exp/tri1/log/build_tree.log
exp/tri1: nj=32 align prob=-79.46 over 150.17h [retry=0.5%, fail=0.0%] states=2080 gauss=20047 tree-impr=4.48
steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri1
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/tri1 exp/tri1_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/tri1, putting alignments in exp/tri1_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri1_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri1_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
steps/train_deltas.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri1_ali exp/tri2
steps/train_deltas.sh: accumulating tree stats
steps/train_deltas.sh: getting questions for tree-building, via clustering
steps/train_deltas.sh: building the tree
steps/train_deltas.sh: converting alignments from exp/tri1_ali to use current tree
steps/train_deltas.sh: compiling graphs of transcripts
steps/train_deltas.sh: training pass 1
steps/train_deltas.sh: training pass 2
steps/train_deltas.sh: training pass 3
steps/train_deltas.sh: training pass 4
steps/train_deltas.sh: training pass 5
steps/train_deltas.sh: training pass 6
steps/train_deltas.sh: training pass 7
steps/train_deltas.sh: training pass 8
steps/train_deltas.sh: training pass 9
steps/train_deltas.sh: training pass 10
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 11
steps/train_deltas.sh: training pass 12
steps/train_deltas.sh: training pass 13
steps/train_deltas.sh: training pass 14
steps/train_deltas.sh: training pass 15
steps/train_deltas.sh: training pass 16
steps/train_deltas.sh: training pass 17
steps/train_deltas.sh: training pass 18
steps/train_deltas.sh: training pass 19
steps/train_deltas.sh: training pass 20
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 21
steps/train_deltas.sh: training pass 22
steps/train_deltas.sh: training pass 23
steps/train_deltas.sh: training pass 24
steps/train_deltas.sh: training pass 25
steps/train_deltas.sh: training pass 26
steps/train_deltas.sh: training pass 27
steps/train_deltas.sh: training pass 28
steps/train_deltas.sh: training pass 29
steps/train_deltas.sh: training pass 30
steps/train_deltas.sh: aligning data
steps/train_deltas.sh: training pass 31
steps/train_deltas.sh: training pass 32
steps/train_deltas.sh: training pass 33
steps/train_deltas.sh: training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri2
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri2/log/analyze_alignments.log
1 warnings in exp/tri2/log/build_tree.log
561 warnings in exp/tri2/log/acc.*.*.log
2190 warnings in exp/tri2/log/align.*.*.log
1 warnings in exp/tri2/log/compile_questions.log
exp/tri2: nj=32 align prob=-79.41 over 150.18h [retry=0.4%, fail=0.0%] states=2120 gauss=20035 tree-impr=4.82
steps/train_deltas.sh: Done training system with delta+delta-delta features in exp/tri2
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/tri2 exp/tri2_ali
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/tri2, putting alignments in exp/tri2_ali
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri2_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri2_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
steps/train_lda_mllt.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri2_ali exp/tri3a
steps/train_lda_mllt.sh: Accumulating LDA statistics.
steps/train_lda_mllt.sh: Accumulating tree stats
steps/train_lda_mllt.sh: Getting questions for tree clustering.
steps/train_lda_mllt.sh: Building the tree
steps/train_lda_mllt.sh: Initializing the model
steps/train_lda_mllt.sh: Converting alignments from exp/tri2_ali to use current tree
steps/train_lda_mllt.sh: Compiling graphs of transcripts
Training pass 1
Training pass 2
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 3
Training pass 4
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 5
Training pass 6
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 7
Training pass 8
Training pass 9
Training pass 10
Aligning data
Training pass 11
Training pass 12
steps/train_lda_mllt.sh: Estimating MLLT
Training pass 13
Training pass 14
Training pass 15
Training pass 16
Training pass 17
Training pass 18
Training pass 19
Training pass 20
Aligning data
Training pass 21
Training pass 22
Training pass 23
Training pass 24
Training pass 25
Training pass 26
Training pass 27
Training pass 28
Training pass 29
Training pass 30
Aligning data
Training pass 31
Training pass 32
Training pass 33
Training pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri3a
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri3a/log/analyze_alignments.log
314 warnings in exp/tri3a/log/acc.*.*.log
1351 warnings in exp/tri3a/log/align.*.*.log
6 warnings in exp/tri3a/log/lda_acc.*.log
1 warnings in exp/tri3a/log/build_tree.log
1 warnings in exp/tri3a/log/compile_questions.log
exp/tri3a: nj=32 align prob=-48.75 over 150.18h [retry=0.3%, fail=0.0%] states=2120 gauss=20039 tree-impr=5.07 lda-sum=24.62 mllt:impr,logdet=0.94,1.40
steps/train_lda_mllt.sh: Done training system with LDA+MLLT features in exp/tri3a
steps/align_fmllr.sh --cmd run.pl --nj 32 data/train data/lang exp/tri3a exp/tri3a_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train using exp/tri3a/final.mdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri3a_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri3a_ali/log/analyze_alignments.log
277 warnings in exp/tri3a_ali/log/align_pass1.*.log
4 warnings in exp/tri3a_ali/log/fmllr.*.log
287 warnings in exp/tri3a_ali/log/align_pass2.*.log
steps/train_sat.sh --cmd run.pl 2500 20000 data/train data/lang exp/tri3a_ali exp/tri4a
steps/train_sat.sh: feature type is lda
steps/train_sat.sh: Using transforms from exp/tri3a_ali
steps/train_sat.sh: Accumulating tree stats
steps/train_sat.sh: Getting questions for tree clustering.
steps/train_sat.sh: Building the tree
steps/train_sat.sh: Initializing the model
steps/train_sat.sh: Converting alignments from exp/tri3a_ali to use current tree
steps/train_sat.sh: Compiling graphs of transcripts
Pass 1
Pass 2
Estimating fMLLR transforms
Pass 3
Pass 4
Estimating fMLLR transforms
Pass 5
Pass 6
Estimating fMLLR transforms
Pass 7
Pass 8
Pass 9
Pass 10
Aligning data
Pass 11
Pass 12
Estimating fMLLR transforms
Pass 13
Pass 14
Pass 15
Pass 16
Pass 17
Pass 18
Pass 19
Pass 20
Aligning data
Pass 21
Pass 22
Pass 23
Pass 24
Pass 25
Pass 26
Pass 27
Pass 28
Pass 29
Pass 30
Aligning data
Pass 31
Pass 32
Pass 33
Pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri4a
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri4a/log/analyze_alignments.log
1 warnings in exp/tri4a/log/build_tree.log
668 warnings in exp/tri4a/log/acc.*.*.log
1 warnings in exp/tri4a/log/compile_questions.log
46 warnings in exp/tri4a/log/fmllr.*.*.log
1599 warnings in exp/tri4a/log/align.*.*.log
steps/train_sat.sh: Likelihood evolution:
-49.2745 -49.0811 -48.9736 -48.8948 -48.4271 -47.9253 -47.5947 -47.3664 -47.1934 -46.8106 -46.6586 -46.4471 -46.3378 -46.2575 -46.1872 -46.1219 -46.0595 -45.9997 -45.9445 -45.8141 -45.748 -45.7056 -45.6685 -45.6343 -45.6021 -45.5709 -45.5403 -45.5111 -45.4831 -45.4135 -45.3743 -45.354 -45.3407 -45.3316 
exp/tri4a: nj=32 align prob=-48.28 over 150.17h [retry=0.3%, fail=0.0%] states=2144 gauss=20029 fmllr-impr=0.63 over 115.52h tree-impr=7.02
steps/train_sat.sh: done training SAT system in exp/tri4a
steps/align_fmllr.sh --cmd run.pl --nj 32 data/train data/lang exp/tri4a exp/tri4a_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train using exp/tri4a/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri4a_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri4a_ali/log/analyze_alignments.log
201 warnings in exp/tri4a_ali/log/align_pass1.*.log
319 warnings in exp/tri4a_ali/log/align_pass2.*.log
steps/train_sat.sh --cmd run.pl 3500 100000 data/train data/lang exp/tri4a_ali exp/tri5a
steps/train_sat.sh: feature type is lda
steps/train_sat.sh: Using transforms from exp/tri4a_ali
steps/train_sat.sh: Accumulating tree stats
steps/train_sat.sh: Getting questions for tree clustering.
steps/train_sat.sh: Building the tree
steps/train_sat.sh: Initializing the model
steps/train_sat.sh: Converting alignments from exp/tri4a_ali to use current tree
steps/train_sat.sh: Compiling graphs of transcripts
Pass 1
Pass 2
Estimating fMLLR transforms
Pass 3
Pass 4
Estimating fMLLR transforms
Pass 5
Pass 6
Estimating fMLLR transforms
Pass 7
Pass 8
Pass 9
Pass 10
Aligning data
Pass 11
Pass 12
Estimating fMLLR transforms
Pass 13
Pass 14
Pass 15
Pass 16
Pass 17
Pass 18
Pass 19
Pass 20
Aligning data
Pass 21
Pass 22
Pass 23
Pass 24
Pass 25
Pass 26
Pass 27
Pass 28
Pass 29
Pass 30
Aligning data
Pass 31
Pass 32
Pass 33
Pass 34
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri5a
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri5a/log/analyze_alignments.log
1 warnings in exp/tri5a/log/compile_questions.log
43 warnings in exp/tri5a/log/fmllr.*.*.log
1 warnings in exp/tri5a/log/build_tree.log
275 warnings in exp/tri5a/log/acc.*.*.log
709 warnings in exp/tri5a/log/align.*.*.log
steps/train_sat.sh: Likelihood evolution:
-48.6116 -48.6563 -48.6109 -48.4513 -47.8757 -47.1559 -46.6018 -46.2344 -45.9676 -45.6512 -45.4835 -45.2222 -45.0957 -45.0029 -44.9192 -44.8426 -44.7733 -44.7109 -44.6531 -44.5344 -44.4648 -44.4177 -44.3761 -44.3378 -44.3015 -44.2667 -44.2331 -44.2009 -44.1699 -44.1059 -44.0629 -44.038 -44.0206 -44.0082 
exp/tri5a: nj=32 align prob=-47.13 over 150.19h [retry=0.1%, fail=0.0%] states=3048 gauss=100108 fmllr-impr=0.25 over 116.58h tree-impr=7.61
steps/train_sat.sh: done training SAT system in exp/tri5a
steps/align_fmllr.sh --cmd run.pl --nj 32 data/train data/lang exp/tri5a exp/tri5a_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train using exp/tri5a/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri5a_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri5a_ali/log/analyze_alignments.log
80 warnings in exp/tri5a_ali/log/align_pass1.*.log
119 warnings in exp/tri5a_ali/log/align_pass2.*.log
-------------------------------------
local/nnet3/run_ivector_common.sh: preparing directory for low-resolution speed-perturbed data (for alignment)
utils/data/perturb_data_dir_speed_3way.sh: making sure the utt2dur and the reco2dur files are present
... in data/train, because obtaining it after speed-perturbing
... would be very slow, and you might need them.
utils/data/get_utt2dur.sh: segments file does not exist so getting durations from wave files
utils/data/get_utt2dur.sh: could not get utterance lengths from sphere-file headers, using wav-to-duration
utils/data/get_utt2dur.sh: computed data/train/utt2dur
utils/data/get_reco2dur.sh: data/train/wav.scp indexed by utt-id; copying utt2dur to reco2dur
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train, in data/train_sp_speed0.9
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_speed0.9
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train, in data/train_sp_speed1.1
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_speed1.1
utils/data/combine_data.sh data/train_sp data/train data/train_sp_speed0.9 data/train_sp_speed1.1
utils/data/combine_data.sh: combined utt2uniq
utils/data/combine_data.sh [info]: not combining segments as it does not exist
utils/data/combine_data.sh: combined utt2spk
utils/data/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/data/combine_data.sh: combined utt2dur
utils/data/combine_data.sh: combined reco2dur
utils/data/combine_data.sh [info]: **not combining feats.scp as it does not exist everywhere**
utils/data/combine_data.sh: combined text
utils/data/combine_data.sh [info]: **not combining cmvn.scp as it does not exist everywhere**
utils/data/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/data/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist
utils/data/combine_data.sh: combined wav.scp
utils/data/combine_data.sh [info]: not combining spk2gender as it does not exist
fix_data_dir.sh: kept all 360294 utterances.
fix_data_dir.sh: old files are kept in data/train_sp/.backup
utils/data/perturb_data_dir_speed_3way.sh: generated 3-way speed-perturbed version of data in data/train, in data/train_sp
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp
local/nnet3/run_ivector_common.sh: making MFCC features for low-resolution speed-perturbed data
steps/make_mfcc_pitch.sh --cmd run.pl --nj 70 data/train_sp exp/make_mfcc/train_sp mfcc_perturbed
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for train_sp
steps/compute_cmvn_stats.sh data/train_sp exp/make_mfcc/train_sp mfcc_perturbed
Succeeded creating CMVN stats for train_sp
fix_data_dir.sh: kept all 360294 utterances.
fix_data_dir.sh: old files are kept in data/train_sp/.backup
local/nnet3/run_ivector_common.sh: aligning with the perturbed low-resolution data
steps/align_fmllr.sh --nj 30 --cmd run.pl data/train_sp data/lang exp/tri5a exp/tri5a_sp_ali
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train_sp using exp/tri5a/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/tri5a_sp_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri5a_sp_ali/log/analyze_alignments.log
415 warnings in exp/tri5a_sp_ali/log/align_pass2.*.log
405 warnings in exp/tri5a_sp_ali/log/align_pass1.*.log
local/nnet3/run_ivector_common.sh: creating high-resolution MFCC features
utils/copy_data_dir.sh: copied data from data/train_sp to data/train_sp_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_hires
utils/copy_data_dir.sh: copied data from data/dev to data/dev_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_hires
utils/copy_data_dir.sh: copied data from data/test to data/test_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/test_hires
utils/data/perturb_data_dir_volume.sh: data/train_sp_hires/feats.scp exists; moving it to data/train_sp_hires/.backup/ as it wouldn't be valid any more.
utils/data/perturb_data_dir_volume.sh: added volume perturbation to the data in data/train_sp_hires
steps/make_mfcc_pitch.sh --nj 10 --mfcc-config conf/mfcc_hires.conf --cmd run.pl data/train_sp_hires exp/make_hires/train_sp mfcc_perturbed_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_hires
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for train_sp_hires
steps/compute_cmvn_stats.sh data/train_sp_hires exp/make_hires/train_sp mfcc_perturbed_hires
Succeeded creating CMVN stats for train_sp_hires
fix_data_dir.sh: kept all 360294 utterances.
fix_data_dir.sh: old files are kept in data/train_sp_hires/.backup
utils/copy_data_dir.sh: copied data from data/train_sp_hires to data/train_sp_hires_nopitch
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_hires_nopitch
utils/data/limit_feature_dim.sh: warning: removing data/train_sp_hires_nopitch/cmvn.cp, you will have to regenerate it from the features.
utils/validate_data_dir.sh: Successfully validated data-directory data/train_sp_hires_nopitch
steps/compute_cmvn_stats.sh data/train_sp_hires_nopitch exp/make_hires/train_sp mfcc_perturbed_hires
Succeeded creating CMVN stats for train_sp_hires_nopitch
steps/make_mfcc_pitch.sh --nj 10 --mfcc-config conf/mfcc_hires.conf --cmd run.pl data/dev_hires exp/make_hires/dev mfcc_perturbed_hires
steps/make_mfcc_pitch.sh: moving data/dev_hires/feats.scp to data/dev_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_hires
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for dev_hires
steps/compute_cmvn_stats.sh data/dev_hires exp/make_hires/dev mfcc_perturbed_hires
Succeeded creating CMVN stats for dev_hires
fix_data_dir.sh: kept all 14326 utterances.
fix_data_dir.sh: old files are kept in data/dev_hires/.backup
utils/copy_data_dir.sh: copied data from data/dev_hires to data/dev_hires_nopitch
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_hires_nopitch
utils/data/limit_feature_dim.sh: warning: removing data/dev_hires_nopitch/cmvn.cp, you will have to regenerate it from the features.
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_hires_nopitch
steps/compute_cmvn_stats.sh data/dev_hires_nopitch exp/make_hires/dev mfcc_perturbed_hires
Succeeded creating CMVN stats for dev_hires_nopitch
steps/make_mfcc_pitch.sh --nj 10 --mfcc-config conf/mfcc_hires.conf --cmd run.pl data/test_hires exp/make_hires/test mfcc_perturbed_hires
steps/make_mfcc_pitch.sh: moving data/test_hires/feats.scp to data/test_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test_hires
steps/make_mfcc_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC & Pitch features for test_hires
steps/compute_cmvn_stats.sh data/test_hires exp/make_hires/test mfcc_perturbed_hires
Succeeded creating CMVN stats for test_hires
fix_data_dir.sh: kept all 7176 utterances.
fix_data_dir.sh: old files are kept in data/test_hires/.backup
utils/copy_data_dir.sh: copied data from data/test_hires to data/test_hires_nopitch
utils/validate_data_dir.sh: Successfully validated data-directory data/test_hires_nopitch
utils/data/limit_feature_dim.sh: warning: removing data/test_hires_nopitch/cmvn.cp, you will have to regenerate it from the features.
utils/validate_data_dir.sh: Successfully validated data-directory data/test_hires_nopitch
steps/compute_cmvn_stats.sh data/test_hires_nopitch exp/make_hires/test mfcc_perturbed_hires
Succeeded creating CMVN stats for test_hires_nopitch
local/nnet3/run_ivector_common.sh: computing a subset of data to train the diagonal UBM.
utils/data/subset_data_dir.sh: reducing #utt from 360294 to 90073
local/nnet3/run_ivector_common.sh: computing a PCA transform from the hires data.
steps/online/nnet2/get_pca_transform.sh --cmd run.pl --splice-opts --left-context=3 --right-context=3 --max-utts 10000 --subsample 2 exp/nnet3/diag_ubm/train_sp_hires_nopitch_subset exp/nnet3/pca_transform
Done estimating PCA transform in exp/nnet3/pca_transform
local/nnet3/run_ivector_common.sh: training the diagonal UBM.
steps/online/nnet2/train_diag_ubm.sh --cmd run.pl --nj 30 --num-frames 700000 --num-threads 8 exp/nnet3/diag_ubm/train_sp_hires_nopitch_subset 512 exp/nnet3/pca_transform exp/nnet3/diag_ubm
steps/online/nnet2/train_diag_ubm.sh: Directory exp/nnet3/diag_ubm already exists. Backing up diagonal UBM in exp/nnet3/diag_ubm/backup.oUB
steps/online/nnet2/train_diag_ubm.sh: initializing model from E-M in memory, 
steps/online/nnet2/train_diag_ubm.sh: starting from 256 Gaussians, reaching 512;
steps/online/nnet2/train_diag_ubm.sh: for 20 iterations, using at most 700000 frames of data
Getting Gaussian-selection info
steps/online/nnet2/train_diag_ubm.sh: will train for 4 iterations, in parallel over
steps/online/nnet2/train_diag_ubm.sh: 30 machines, parallelized with 'run.pl'
steps/online/nnet2/train_diag_ubm.sh: Training pass 0
steps/online/nnet2/train_diag_ubm.sh: Training pass 1
steps/online/nnet2/train_diag_ubm.sh: Training pass 2
steps/online/nnet2/train_diag_ubm.sh: Training pass 3
local/nnet3/run_ivector_common.sh: training the iVector extractor
steps/online/nnet2/train_ivector_extractor.sh --cmd run.pl --nj 10 data/train_sp_hires_nopitch exp/nnet3/diag_ubm exp/nnet3/extractor
steps/online/nnet2/train_ivector_extractor.sh: doing Gaussian selection and posterior computation
Accumulating stats (pass 0)
Summing accs (pass 0)
Updating model (pass 0)
Accumulating stats (pass 1)
Summing accs (pass 1)
Updating model (pass 1)
Accumulating stats (pass 2)
Summing accs (pass 2)
Updating model (pass 2)
Accumulating stats (pass 3)
Summing accs (pass 3)
Updating model (pass 3)
Accumulating stats (pass 4)
Summing accs (pass 4)
Updating model (pass 4)
Accumulating stats (pass 5)
Summing accs (pass 5)
Updating model (pass 5)
Accumulating stats (pass 6)
Summing accs (pass 6)
Updating model (pass 6)
Accumulating stats (pass 7)
Summing accs (pass 7)
Updating model (pass 7)
Accumulating stats (pass 8)
Summing accs (pass 8)
Updating model (pass 8)
Accumulating stats (pass 9)
Summing accs (pass 9)
Updating model (pass 9)
utils/data/modify_speaker_info.sh: copied data from data/train_sp_hires_nopitch to exp/nnet3/ivectors_train_sp/train_sp_sp_hires_nopitch_max2, number of speakers changed from 1020 to 180399
utils/validate_data_dir.sh: Successfully validated data-directory exp/nnet3/ivectors_train_sp/train_sp_sp_hires_nopitch_max2
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 30 exp/nnet3/ivectors_train_sp/train_sp_sp_hires_nopitch_max2 exp/nnet3/extractor exp/nnet3/ivectors_train_sp
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3/ivectors_train_sp using the extractor in exp/nnet3/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 8 data/dev_hires_nopitch exp/nnet3/extractor exp/nnet3/ivectors_dev
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3/ivectors_dev using the extractor in exp/nnet3/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd run.pl --nj 8 data/test_hires_nopitch exp/nnet3/extractor exp/nnet3/ivectors_test
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3/ivectors_test using the extractor in exp/nnet3/extractor.
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/tri5a_sp_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_sp/configs/network.xconfig --config-dir exp/nnet3/tdnn_sp/configs/
nnet3-init exp/nnet3/tdnn_sp/configs//init.config exp/nnet3/tdnn_sp/configs//init.raw 
LOG (nnet3-init[5.5.266~1-77ac7]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_sp/configs//init.raw
nnet3-info exp/nnet3/tdnn_sp/configs//init.raw 
nnet3-init exp/nnet3/tdnn_sp/configs//ref.config exp/nnet3/tdnn_sp/configs//ref.raw 
LOG (nnet3-init[5.5.266~1-77ac7]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_sp/configs//ref.raw
nnet3-info exp/nnet3/tdnn_sp/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_sp/configs//ref.config exp/nnet3/tdnn_sp/configs//ref.raw 
LOG (nnet3-init[5.5.266~1-77ac7]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_sp/configs//ref.raw
nnet3-info exp/nnet3/tdnn_sp/configs//ref.raw 
cat: data/_hires/utt2spk: No such file or directory
this shell script execution duration: 10510 s
-------------------------------------
tree-info exp/tri5a_sp_ali/tree 
Traceback (most recent call last):
  File "/usr/lib64/python2.7/pdb.py", line 1314, in main
    pdb._runscript(mainpyfile)
  File "/usr/lib64/python2.7/pdb.py", line 1233, in _runscript
    self.run(statement)
  File "/usr/lib64/python2.7/bdb.py", line 400, in run
    exec cmd in globals, locals
  File "<string>", line 1, in <module>
  File "steps/nnet3/train_dnn.py", line 176
    pdb.set_trace()
   ^
IndentationError: unexpected indent
-------------------------------------
tree-info exp/tri5a_sp_ali/tree 
steps/nnet3/train_dnn.py: line 10:  This script is based on steps/nnet3/tdnn/train.sh
: No such file or directory
steps/nnet3/train_dnn.py: line 12: from: command not found
steps/nnet3/train_dnn.py: line 13: from: command not found
steps/nnet3/train_dnn.py: line 14: import: command not found
steps/nnet3/train_dnn.py: line 15: import: command not found
steps/nnet3/train_dnn.py: line 16: import: command not found
steps/nnet3/train_dnn.py: line 17: import: command not found
steps/nnet3/train_dnn.py: line 18: import: command not found
steps/nnet3/train_dnn.py: line 19: import: command not found
steps/nnet3/train_dnn.py: line 20: import: command not found
steps/nnet3/train_dnn.py: line 22: syntax error near unexpected token `0,'
steps/nnet3/train_dnn.py: line 22: `sys.path.insert(0, 'steps')'
this shell script execution duration: 0 s
-------------------------------------
tree-info exp/tri5a_sp_ali/tree 
steps/nnet3/train_dnn.py: line 9:  This script is based on steps/nnet3/tdnn/train.sh: No such file or directory
steps/nnet3/train_dnn.py: line 11: from: command not found
steps/nnet3/train_dnn.py: line 12: from: command not found
steps/nnet3/train_dnn.py: line 13: import: command not found
steps/nnet3/train_dnn.py: line 14: import: command not found
steps/nnet3/train_dnn.py: line 15: import: command not found
steps/nnet3/train_dnn.py: line 16: import: command not found
steps/nnet3/train_dnn.py: line 17: import: command not found
steps/nnet3/train_dnn.py: line 18: import: command not found
steps/nnet3/train_dnn.py: line 19: import: command not found
steps/nnet3/train_dnn.py: line 21: syntax error near unexpected token `0,'
steps/nnet3/train_dnn.py: line 21: `sys.path.insert(0, 'steps')'
this shell script execution duration: 1 s
-------------------------------------
tree-info exp/tri5a_sp_ali/tree 
steps/nnet3/train_dnn.py: line 10:  This script is based on steps/nnet3/tdnn/train.sh
: No such file or directory
steps/nnet3/train_dnn.py: line 12: from: command not found
steps/nnet3/train_dnn.py: line 13: from: command not found
steps/nnet3/train_dnn.py: line 14: import: command not found
steps/nnet3/train_dnn.py: line 15: import: command not found
steps/nnet3/train_dnn.py: line 16: import: command not found
steps/nnet3/train_dnn.py: line 17: import: command not found
steps/nnet3/train_dnn.py: line 18: import: command not found
steps/nnet3/train_dnn.py: line 19: import: command not found
steps/nnet3/train_dnn.py: line 20: import: command not found
steps/nnet3/train_dnn.py: line 22: syntax error near unexpected token `0,'
steps/nnet3/train_dnn.py: line 22: `sys.path.insert(0, 'steps')'
this shell script execution duration: 0 s
-------------------------------------
tree-info exp/tri5a_sp_ali/tree 
2019-08-16 16:15:45,153 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=-10 --cmd=run.pl --feat.online-ivector-dir exp/nnet3/ivectors_train_sp --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 2 --trainer.optimization.num-jobs-final 12 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu true --feat-dir=data/train_sp_hires --ali-dir exp/tri5a_sp_ali --lang data/lang --reporting.email= --dir=exp/nnet3/tdnn_sp
['steps/nnet3/train_dnn.py', '--stage=-10', '--cmd=run.pl', '--feat.online-ivector-dir', 'exp/nnet3/ivectors_train_sp', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '2', '--trainer.optimization.num-jobs-final', '12', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'true', '--feat-dir=data/train_sp_hires', '--ali-dir', 'exp/tri5a_sp_ali', '--lang', 'data/lang', '--reporting.email=', '--dir=exp/nnet3/tdnn_sp']
2019-08-16 16:15:45,284 [steps/nnet3/train_dnn.py:178 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/tri5a_sp_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_sp',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train_sp_hires',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '512',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 12,
 'num_jobs_initial': 2,
 'online_ivector_dir': 'exp/nnet3/ivectors_train_sp',
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-16 16:15:46,398 [steps/nnet3/train_dnn.py:229 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-16 16:15:47,003 [steps/nnet3/train_dnn.py:239 - train - INFO ] Generating egs
----------get_egs.sh----------------
steps/nnet3/get_egs.sh --cmd run.pl --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir exp/nnet3/ivectors_train_sp --left-context 16 --right-context 12 --left-context-initial -1 --right-context-final -1 --stage 0 --samples-per-iter 400000 --frames-per-eg 8 --srand 0 data/train_sp_hires exp/tri5a_sp_ali exp/nnet3/tdnn_sp/egs
File data/train_sp_hires/utt2uniq exists, so augmenting valid_uttlist to
include all perturbed versions of the same 'real' utterances.
steps/nnet3/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/nnet3/tdnn_sp/egs/.nodelete
steps/nnet3/get_egs.sh: feature type is raw
feat-to-dim scp:exp/nnet3/ivectors_train_sp/ivector_online.scp - 
steps/nnet3/get_egs.sh: working out number of frames of training data
steps/nnet3/get_egs.sh: working out feature dim
steps/nnet3/get_egs.sh: creating 52 archives, each with 394272 egs, with
steps/nnet3/get_egs.sh:   8 labels per example, and (left,right) context = (16,12)
steps/nnet3/get_egs.sh: copying data alignments
copy-int-vector ark:- ark,scp:exp/nnet3/tdnn_sp/egs/ali.ark,exp/nnet3/tdnn_sp/egs/ali.scp 
LOG (copy-int-vector[5.5.266~1-77ac7]:main():copy-int-vector.cc:83) Copied 360293 vectors of int32.
steps/nnet3/get_egs.sh: Getting validation and training subset examples.
steps/nnet3/get_egs.sh: ... extracting validation and training-subset alignments.
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/get_egs.sh: Generating training examples on disk
steps/nnet3/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/get_egs.sh: removing temporary archives
steps/nnet3/get_egs.sh: removing temporary alignments
steps/nnet3/get_egs.sh: Finished preparing training examples
cat: data/_hires/utt2spk: No such file or directory
this shell script execution duration: 1636 s
-------------------------------------
local/nnet3/run_tdnn.sh: line 44: dir-exp/nnet3/tdnn: No such file or directory
this shell script execution duration: 0 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/tri5a_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn/configs/network.xconfig --config-dir exp/nnet3/tdnn/configs/
nnet3-init exp/nnet3/tdnn/configs//init.config exp/nnet3/tdnn/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn/configs//init.raw
nnet3-info exp/nnet3/tdnn/configs//init.raw 
nnet3-init exp/nnet3/tdnn/configs//ref.config exp/nnet3/tdnn/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn/configs//ref.raw
nnet3-info exp/nnet3/tdnn/configs//ref.raw 
nnet3-init exp/nnet3/tdnn/configs//ref.config exp/nnet3/tdnn/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn/configs//ref.raw
nnet3-info exp/nnet3/tdnn/configs//ref.raw 
cat: data/_hires/utt2spk: No such file or directory
this shell script execution duration: 2 s
-------------------------------------
tree-info exp/tri5a_ali/tree 
-------------------------------------
this shell script execution duration: 0 s
-------------------------------------
> /home/data/yelong/kaldi/egs/aishell/s5/steps/nnet3/train_dnn.py(9)<module>()
-> """
(Pdb) Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
> /usr/lib64/python2.7/cmd.py(130)cmdloop()
-> line = raw_input(self.prompt)
(Pdb) this shell script execution duration: 6 s
-------------------------------------
tree-info exp/tri5a_ali/tree 
steps/nnet3/decode.sh --nj 40 --cmd run.pl --online-ivector-dir exp/nnet3/ivectors_dev exp/tri5a/graph data/dev exp/nnet3/tdnn/decode_dev
steps/nnet2/check_ivectors_compatible.sh: WARNING: One of the directories do not contain iVector ID.
steps/nnet2/check_ivectors_compatible.sh: WARNING: That means it's you who's reponsible for keeping 
steps/nnet2/check_ivectors_compatible.sh: WARNING: the directories compatible
steps/nnet3/decode.sh: no such file exp/tri5a/graph/HCLG.fst
this shell script execution duration: 1 s
-------------------------------------
tree-info exp/tri5a/tree 
tree-info exp/tri5a/tree 
fstminimizeencoded 
fsttablecompose data/lang_test/L_disambig.fst data/lang_test/G.fst 
fstdeterminizestar --use-log=true 
fstpushspecial 
fstisstochastic data/lang_test/tmp/LG.fst 
-0.0663446 -0.0666824
[info]: LG not stochastic.
fstcomposecontext --context-size=3 --central-position=1 --read-disambig-syms=data/lang_test/phones/disambig.int --write-disambig-syms=data/lang_test/tmp/disambig_ilabels_3_1.int data/lang_test/tmp/ilabels_3_1.55216 data/lang_test/tmp/LG.fst 
fstisstochastic data/lang_test/tmp/CLG_3_1.fst 
0 -0.0666824
[info]: CLG not stochastic.
make-h-transducer --disambig-syms-out=exp/tri5a/graph/disambig_tid.int --transition-scale=1.0 data/lang_test/tmp/ilabels_3_1 exp/tri5a/tree exp/tri5a/final.mdl 
fstminimizeencoded 
fsttablecompose exp/tri5a/graph/Ha.fst data/lang_test/tmp/CLG_3_1.fst 
fstdeterminizestar --use-log=true 
fstrmepslocal 
fstrmsymbols exp/tri5a/graph/disambig_tid.int 
fstisstochastic exp/tri5a/graph/HCLGa.fst 
0.000487832 -0.178947
HCLGa is not stochastic
add-self-loops --self-loop-scale=0.1 --reorder=true exp/tri5a/final.mdl exp/tri5a/graph/HCLGa.fst 
steps/decode_fmllr.sh --cmd run.pl --nj 10 --config conf/decode.config exp/tri5a/graph data/dev exp/tri5a/decode_dev
steps/decode.sh --scoring-opts  --num-threads 1 --skip-scoring false --acwt 0.083333 --nj 10 --cmd run.pl --beam 8.0 --model exp/tri5a/final.alimdl --max-active 2000 exp/tri5a/graph data/dev exp/tri5a/decode_dev.si
decode.sh: feature type is lda
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_dev.si
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_dev.si/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,14) and mean=5.7
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_dev.si/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev.si
steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev.si
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev.si
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev.si
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/decode_fmllr.sh: feature type is lda
steps/decode_fmllr.sh: getting first-pass fMLLR transforms.
steps/decode_fmllr.sh: doing main lattice generation phase
steps/decode_fmllr.sh: estimating fMLLR transforms a second time.
steps/decode_fmllr.sh: doing a final pass of acoustic rescoring.
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_dev
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_dev/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,2,15) and mean=6.2
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_dev/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev
steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/tri5a/decode_dev
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/decode_fmllr.sh --cmd run.pl --nj 10 --config conf/decode.config exp/tri5a/graph data/test exp/tri5a/decode_test
steps/decode.sh --scoring-opts  --num-threads 1 --skip-scoring false --acwt 0.083333 --nj 10 --cmd run.pl --beam 8.0 --model exp/tri5a/final.alimdl --max-active 2000 exp/tri5a/graph data/test exp/tri5a/decode_test.si
decode.sh: feature type is lda
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_test.si
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test.si/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,3,16) and mean=6.8
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test.si/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test.si
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
steps/decode_fmllr.sh: feature type is lda
steps/decode_fmllr.sh: getting first-pass fMLLR transforms.
steps/decode_fmllr.sh: doing main lattice generation phase
steps/decode_fmllr.sh: estimating fMLLR transforms a second time.
steps/decode_fmllr.sh: doing a final pass of acoustic rescoring.
steps/diagnostic/analyze_lats.sh --cmd run.pl exp/tri5a/graph exp/tri5a/decode_test
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,3,18) and mean=7.4
steps/diagnostic/analyze_lats.sh: see stats in exp/tri5a/decode_test/log/analyze_lattice_depth_stats.log
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/tri5a/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
this shell script execution duration: 3010 s
-------------------------------------
tree-info exp/tri5a_ali/tree 
steps/nnet3/decode.sh --nj 40 --cmd run.pl --online-ivector-dir exp/nnet3/ivectors_dev exp/tri5a/graph data/dev exp/nnet3/tdnn/decode_dev
steps/nnet2/check_ivectors_compatible.sh: WARNING: One of the directories do not contain iVector ID.
steps/nnet2/check_ivectors_compatible.sh: WARNING: That means it's you who's reponsible for keeping 
steps/nnet2/check_ivectors_compatible.sh: WARNING: the directories compatible
steps/nnet3/decode.sh: no such file exp/nnet3/tdnn/final.mdl

-------------------------------------
tree-info exp/tri5a_ali/tree 
2019-08-22 09:10:36,609 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=-10 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 4 --trainer.optimization.num-jobs-final 4 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/tri5a_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 512 --dir=exp/nnet3/tdnn
['steps/nnet3/train_dnn.py', '--stage=-10', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '4', '--trainer.optimization.num-jobs-final', '4', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/tri5a_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '512', '--dir=exp/nnet3/tdnn']
2019-08-22 09:10:36,628 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/tri5a_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '512',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 4,
 'num_jobs_initial': 4,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-22 09:10:36,708 [steps/nnet3/train_dnn.py:230 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-22 09:10:36,769 [steps/nnet3/train_dnn.py:240 - train - INFO ] Generating egs
----------get_egs.sh----------------
steps/nnet3/get_egs.sh --cmd run.pl --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir  --left-context 16 --right-context 12 --left-context-initial -1 --right-context-final -1 --stage 0 --samples-per-iter 400000 --frames-per-eg 8 --srand 0 data/train exp/tri5a_ali exp/nnet3/tdnn/egs
steps/nnet3/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/nnet3/tdnn/egs/.nodelete
steps/nnet3/get_egs.sh: feature type is raw
steps/nnet3/get_egs.sh: working out number of frames of training data
steps/nnet3/get_egs.sh: working out feature dim
steps/nnet3/get_egs.sh: creating 17 archives, each with 399314 egs, with
steps/nnet3/get_egs.sh:   8 labels per example, and (left,right) context = (16,12)
steps/nnet3/get_egs.sh: copying data alignments
copy-int-vector ark:- ark,scp:exp/nnet3/tdnn/egs/ali.ark,exp/nnet3/tdnn/egs/ali.scp 
LOG (copy-int-vector[5.5.458~2-84ab]:main():copy-int-vector.cc:83) Copied 120098 vectors of int32.
steps/nnet3/get_egs.sh: Getting validation and training subset examples.
steps/nnet3/get_egs.sh: ... extracting validation and training-subset alignments.
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/get_egs.sh: Generating training examples on disk
steps/nnet3/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/get_egs.sh: removing temporary archives
steps/nnet3/get_egs.sh: removing temporary alignments
steps/nnet3/get_egs.sh: Finished preparing training examples
2019-08-22 09:12:03,882 [steps/nnet3/train_dnn.py:276 - train - INFO ] Computing the preconditioning matrix for input features
cat: data//utt2spk: No such file or directory
this shell script execution duration: 98 s
-------------------------------------
tree-info exp/tri5a_ali/tree 
steps/nnet3/decode.sh --nj 40 --cmd run.pl exp/tri5a/graph data/dev exp/nnet3/tdnn/decode_dev
steps/nnet3/decode.sh: feature type is raw
steps/diagnostic/analyze_lats.sh --cmd run.pl --iter final exp/tri5a/graph exp/nnet3/tdnn/decode_dev
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn/decode_dev/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,3,24) and mean=10.2
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn/decode_dev/log/analyze_lattice_depth_stats.log
score best paths
+ steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn/decode_dev
steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn/decode_dev
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn/decode_dev
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn/decode_dev
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
score confidence and timing with sclite
Decoding done.
steps/nnet3/decode.sh --nj 20 --cmd run.pl exp/tri5a/graph data/test exp/nnet3/tdnn/decode_test
steps/nnet3/decode.sh: feature type is raw
steps/diagnostic/analyze_lats.sh --cmd run.pl --iter final exp/tri5a/graph exp/nnet3/tdnn/decode_test
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,4,33) and mean=14.1
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn/decode_test/log/analyze_lattice_depth_stats.log
score best paths
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
score confidence and timing with sclite
Decoding done.
this shell script execution duration: 1642 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
steps/align_si.sh: done aligning data.
this shell script execution duration: 0 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: done aligning data.
this shell script execution duration: 0 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
steps/align_si.sh: done aligning data.
this shell script execution duration: 0 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
steps/align_si.sh: done aligning data.
this shell script execution duration: 0 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
run.pl: 32 / 32 failed, log is in exp/nnet3/tdnn_ali/log/align.*.log
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
run.pl: 32 / 32 failed, log is in exp/nnet3/tdnn_ali/log/align.*.log
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
run.pl: 32 / 32 failed, log is in exp/nnet3/tdnn_ali/log/align.*.log
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
run.pl: 32 / 32 failed, log is in exp/nnet3/tdnn_ali/log/align.*.log
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/align_si.sh: feature type is delta
steps/align_si.sh: aligning data in data/train using model from exp/nnet3/tdnn, putting alignments in exp/nnet3/tdnn_ali
steps/align_si.sh: done aligning data.
this shell script execution duration: 1869 s
-------------------------------------
steps/align_si.sh --cmd run.pl --nj 32 data/train data/lang exp/nnet3/tdnn exp/nnet3/tdnn_ali
cp: cannot stat 'exp/nnet3/tdnn/final.occs': No such file or directory
steps/diagnostic/analyze_alignments.sh --cmd run.pl data/lang exp/nnet3/tdnn_ali
steps/diagnostic/analyze_alignments.sh: see stats in exp/nnet3/tdnn_ali/log/analyze_alignments.log
steps/align_si.sh: done aligning data.
this shell script execution duration: 3 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new/configs/network.xconfig --config-dir exp/nnet3/tdnn_new/configs/
nnet3-init exp/nnet3/tdnn_new/configs//init.config exp/nnet3/tdnn_new/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//init.raw
nnet3-info exp/nnet3/tdnn_new/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
2019-08-22 12:37:53,940 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=-10 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 4 --trainer.optimization.num-jobs-final 4 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/nnet3/tdnn_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new
['steps/nnet3/train_dnn.py', '--stage=-10', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '4', '--trainer.optimization.num-jobs-final', '4', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/nnet3/tdnn_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new']
2019-08-22 12:37:53,959 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/nnet3/tdnn_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 4,
 'num_jobs_initial': 4,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-22 12:37:53,993 [steps/nnet3/train_dnn.py:230 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-22 12:37:54,039 [steps/nnet3/train_dnn.py:240 - train - INFO ] Generating egs
----------get_egs.sh----------------
steps/nnet3/get_egs.sh --cmd run.pl --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir  --left-context 16 --right-context 12 --left-context-initial -1 --right-context-final -1 --stage 0 --samples-per-iter 400000 --frames-per-eg 8 --srand 0 data/train exp/nnet3/tdnn_ali exp/nnet3/tdnn_new/egs
steps/nnet3/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/nnet3/tdnn_new/egs/.nodelete
steps/nnet3/get_egs.sh: feature type is raw
steps/nnet3/get_egs.sh: working out number of frames of training data
steps/nnet3/get_egs.sh: working out feature dim
steps/nnet3/get_egs.sh: creating 17 archives, each with 399314 egs, with
steps/nnet3/get_egs.sh:   8 labels per example, and (left,right) context = (16,12)
steps/nnet3/get_egs.sh: copying data alignments
copy-int-vector ark:- ark,scp:exp/nnet3/tdnn_new/egs/ali.ark,exp/nnet3/tdnn_new/egs/ali.scp 
LOG (copy-int-vector[5.5.458~2-84ab]:main():copy-int-vector.cc:83) Copied 120098 vectors of int32.
steps/nnet3/get_egs.sh: Getting validation and training subset examples.
steps/nnet3/get_egs.sh: ... extracting validation and training-subset alignments.
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/get_egs.sh: Generating training examples on disk
steps/nnet3/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/get_egs.sh: removing temporary archives
steps/nnet3/get_egs.sh: removing temporary alignments
steps/nnet3/get_egs.sh: Finished preparing training examples
2019-08-22 12:39:26,917 [steps/nnet3/train_dnn.py:276 - train - INFO ] Computing the preconditioning matrix for input features
2019-08-22 12:39:36,215 [steps/nnet3/train_dnn.py:287 - train - INFO ] Computing initial vector for FixedScaleComponent before softmax, using priors^-0.25 and rescaling to average 1
2019-08-22 12:39:38,036 [steps/nnet3/train_dnn.py:293 - train - INFO ] Preparing the initial acoustic model.
2019-08-22 12:39:45,719 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 136 iterations
2019-08-22 12:39:45,720 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 0/135   Jobs: 4   Epoch: 0.00/4.0 (0.0% complete)   lr: 0.006000   
2019-08-22 12:40:47,489 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 1/135   Jobs: 4   Epoch: 0.03/4.0 (0.7% complete)   lr: 0.005899   
2019-08-22 12:41:37,644 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 2/135   Jobs: 4   Epoch: 0.06/4.0 (1.5% complete)   lr: 0.005800   
2019-08-22 12:42:28,085 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 3/135   Jobs: 4   Epoch: 0.09/4.0 (2.2% complete)   lr: 0.005703   
2019-08-22 12:43:18,882 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 4/135   Jobs: 4   Epoch: 0.12/4.0 (2.9% complete)   lr: 0.005607   
2019-08-22 12:44:10,487 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 5/135   Jobs: 4   Epoch: 0.15/4.0 (3.7% complete)   lr: 0.005513   
2019-08-22 12:45:01,878 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 6/135   Jobs: 4   Epoch: 0.18/4.0 (4.4% complete)   lr: 0.005420   
2019-08-22 12:45:53,843 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 7/135   Jobs: 4   Epoch: 0.21/4.0 (5.1% complete)   lr: 0.005329   
2019-08-22 12:46:45,898 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 8/135   Jobs: 4   Epoch: 0.24/4.0 (5.9% complete)   lr: 0.005240   
2019-08-22 12:47:37,493 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 9/135   Jobs: 4   Epoch: 0.26/4.0 (6.6% complete)   lr: 0.005152   
2019-08-22 12:48:29,336 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 10/135   Jobs: 4   Epoch: 0.29/4.0 (7.4% complete)   lr: 0.005065   
2019-08-22 12:49:21,471 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 11/135   Jobs: 4   Epoch: 0.32/4.0 (8.1% complete)   lr: 0.004980   
2019-08-22 12:50:13,339 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 12/135   Jobs: 4   Epoch: 0.35/4.0 (8.8% complete)   lr: 0.004897   
2019-08-22 12:51:05,559 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 13/135   Jobs: 4   Epoch: 0.38/4.0 (9.6% complete)   lr: 0.004815   
2019-08-22 12:51:57,878 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 14/135   Jobs: 4   Epoch: 0.41/4.0 (10.3% complete)   lr: 0.004734   
2019-08-22 12:52:49,823 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 15/135   Jobs: 4   Epoch: 0.44/4.0 (11.0% complete)   lr: 0.004654   
2019-08-22 12:53:43,192 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 16/135   Jobs: 4   Epoch: 0.47/4.0 (11.8% complete)   lr: 0.004576   
2019-08-22 12:54:35,254 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 17/135   Jobs: 4   Epoch: 0.50/4.0 (12.5% complete)   lr: 0.004499   
2019-08-22 12:55:27,105 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 18/135   Jobs: 4   Epoch: 0.53/4.0 (13.2% complete)   lr: 0.004424   
2019-08-22 12:56:18,830 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 19/135   Jobs: 4   Epoch: 0.56/4.0 (14.0% complete)   lr: 0.004350   
2019-08-22 12:57:10,073 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 20/135   Jobs: 4   Epoch: 0.59/4.0 (14.7% complete)   lr: 0.004277   
2019-08-22 12:58:09,322 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 21/135   Jobs: 4   Epoch: 0.62/4.0 (15.4% complete)   lr: 0.004205   
2019-08-22 12:59:02,122 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 22/135   Jobs: 4   Epoch: 0.65/4.0 (16.2% complete)   lr: 0.004134   
2019-08-22 13:00:00,682 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 23/135   Jobs: 4   Epoch: 0.68/4.0 (16.9% complete)   lr: 0.004065   
2019-08-22 13:01:02,576 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 24/135   Jobs: 4   Epoch: 0.71/4.0 (17.6% complete)   lr: 0.003997   
2019-08-22 13:02:04,135 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 25/135   Jobs: 4   Epoch: 0.74/4.0 (18.4% complete)   lr: 0.003929   
2019-08-22 13:02:56,641 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 26/135   Jobs: 4   Epoch: 0.76/4.0 (19.1% complete)   lr: 0.003863   
2019-08-22 13:03:49,872 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 27/135   Jobs: 4   Epoch: 0.79/4.0 (19.9% complete)   lr: 0.003799   
2019-08-22 13:04:42,242 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 28/135   Jobs: 4   Epoch: 0.82/4.0 (20.6% complete)   lr: 0.003735   
2019-08-22 13:05:35,508 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 29/135   Jobs: 4   Epoch: 0.85/4.0 (21.3% complete)   lr: 0.003672   
2019-08-22 13:06:42,879 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 30/135   Jobs: 4   Epoch: 0.88/4.0 (22.1% complete)   lr: 0.003610   
2019-08-22 13:07:44,079 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 31/135   Jobs: 4   Epoch: 0.91/4.0 (22.8% complete)   lr: 0.003550   
2019-08-22 13:08:38,813 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 32/135   Jobs: 4   Epoch: 0.94/4.0 (23.5% complete)   lr: 0.003490   
2019-08-22 13:09:30,816 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 33/135   Jobs: 4   Epoch: 0.97/4.0 (24.3% complete)   lr: 0.003432   
2019-08-22 13:10:24,363 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 34/135   Jobs: 4   Epoch: 1.00/4.0 (25.0% complete)   lr: 0.003374   
2019-08-22 13:11:16,288 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 35/135   Jobs: 4   Epoch: 1.03/4.0 (25.7% complete)   lr: 0.003317   
2019-08-22 13:12:13,055 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 36/135   Jobs: 4   Epoch: 1.06/4.0 (26.5% complete)   lr: 0.003262   
2019-08-22 13:13:15,448 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 37/135   Jobs: 4   Epoch: 1.09/4.0 (27.2% complete)   lr: 0.003207   
2019-08-22 13:14:17,152 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 38/135   Jobs: 4   Epoch: 1.12/4.0 (27.9% complete)   lr: 0.003153   
2019-08-22 13:15:09,031 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 39/135   Jobs: 4   Epoch: 1.15/4.0 (28.7% complete)   lr: 0.003100   
2019-08-22 13:16:01,058 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 40/135   Jobs: 4   Epoch: 1.18/4.0 (29.4% complete)   lr: 0.003048   
2019-08-22 13:16:59,062 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 41/135   Jobs: 4   Epoch: 1.21/4.0 (30.1% complete)   lr: 0.002997   
2019-08-22 13:17:52,181 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 42/135   Jobs: 4   Epoch: 1.24/4.0 (30.9% complete)   lr: 0.002947   
2019-08-22 13:18:56,677 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 43/135   Jobs: 4   Epoch: 1.26/4.0 (31.6% complete)   lr: 0.002897   
2019-08-22 13:19:58,399 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 44/135   Jobs: 4   Epoch: 1.29/4.0 (32.4% complete)   lr: 0.002849   
2019-08-22 13:20:55,466 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 45/135   Jobs: 4   Epoch: 1.32/4.0 (33.1% complete)   lr: 0.002801   
2019-08-22 13:21:47,683 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 46/135   Jobs: 4   Epoch: 1.35/4.0 (33.8% complete)   lr: 0.002754   
2019-08-22 13:22:40,525 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 47/135   Jobs: 4   Epoch: 1.38/4.0 (34.6% complete)   lr: 0.002707   
2019-08-22 13:23:42,888 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 48/135   Jobs: 4   Epoch: 1.41/4.0 (35.3% complete)   lr: 0.002662   
2019-08-22 13:24:47,620 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 49/135   Jobs: 4   Epoch: 1.44/4.0 (36.0% complete)   lr: 0.002617   
2019-08-22 13:25:45,120 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 50/135   Jobs: 4   Epoch: 1.47/4.0 (36.8% complete)   lr: 0.002573   
2019-08-22 13:26:37,082 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 51/135   Jobs: 4   Epoch: 1.50/4.0 (37.5% complete)   lr: 0.002530   
2019-08-22 13:27:29,126 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 52/135   Jobs: 4   Epoch: 1.53/4.0 (38.2% complete)   lr: 0.002488   
2019-08-22 13:28:21,755 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 53/135   Jobs: 4   Epoch: 1.56/4.0 (39.0% complete)   lr: 0.002446   
2019-08-22 13:29:14,124 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 54/135   Jobs: 4   Epoch: 1.59/4.0 (39.7% complete)   lr: 0.002405   
2019-08-22 13:30:06,764 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 55/135   Jobs: 4   Epoch: 1.62/4.0 (40.4% complete)   lr: 0.002365   
2019-08-22 13:30:58,906 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 56/135   Jobs: 4   Epoch: 1.65/4.0 (41.2% complete)   lr: 0.002325   
2019-08-22 13:31:51,522 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 57/135   Jobs: 4   Epoch: 1.68/4.0 (41.9% complete)   lr: 0.002286   
2019-08-22 13:32:44,066 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 58/135   Jobs: 4   Epoch: 1.71/4.0 (42.6% complete)   lr: 0.002247   
2019-08-22 13:33:36,170 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 59/135   Jobs: 4   Epoch: 1.74/4.0 (43.4% complete)   lr: 0.002210   
2019-08-22 13:34:28,514 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 60/135   Jobs: 4   Epoch: 1.76/4.0 (44.1% complete)   lr: 0.002173   
2019-08-22 13:35:27,823 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 61/135   Jobs: 4   Epoch: 1.79/4.0 (44.9% complete)   lr: 0.002136   
2019-08-22 13:36:20,378 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 62/135   Jobs: 4   Epoch: 1.82/4.0 (45.6% complete)   lr: 0.002100   
2019-08-22 13:37:12,578 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 63/135   Jobs: 4   Epoch: 1.85/4.0 (46.3% complete)   lr: 0.002065   
2019-08-22 13:38:06,791 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 64/135   Jobs: 4   Epoch: 1.88/4.0 (47.1% complete)   lr: 0.002030   
2019-08-22 13:39:00,566 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 65/135   Jobs: 4   Epoch: 1.91/4.0 (47.8% complete)   lr: 0.001996   
2019-08-22 13:39:54,375 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 66/135   Jobs: 4   Epoch: 1.94/4.0 (48.5% complete)   lr: 0.001963   
2019-08-22 13:40:48,248 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 67/135   Jobs: 4   Epoch: 1.97/4.0 (49.3% complete)   lr: 0.001930   
2019-08-22 13:41:42,067 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 68/135   Jobs: 4   Epoch: 2.00/4.0 (50.0% complete)   lr: 0.001897   
2019-08-22 13:42:35,759 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 69/135   Jobs: 4   Epoch: 2.03/4.0 (50.7% complete)   lr: 0.001866   
2019-08-22 13:43:29,514 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 70/135   Jobs: 4   Epoch: 2.06/4.0 (51.5% complete)   lr: 0.001834   
2019-08-22 13:44:23,204 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 71/135   Jobs: 4   Epoch: 2.09/4.0 (52.2% complete)   lr: 0.001803   
2019-08-22 13:45:16,791 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 72/135   Jobs: 4   Epoch: 2.12/4.0 (52.9% complete)   lr: 0.001773   
2019-08-22 13:46:10,438 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 73/135   Jobs: 4   Epoch: 2.15/4.0 (53.7% complete)   lr: 0.001743   
2019-08-22 13:47:04,192 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 74/135   Jobs: 4   Epoch: 2.18/4.0 (54.4% complete)   lr: 0.001714   
2019-08-22 13:47:58,211 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 75/135   Jobs: 4   Epoch: 2.21/4.0 (55.1% complete)   lr: 0.001685   
2019-08-22 13:48:52,064 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 76/135   Jobs: 4   Epoch: 2.24/4.0 (55.9% complete)   lr: 0.001657   
2019-08-22 13:49:45,916 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 77/135   Jobs: 4   Epoch: 2.26/4.0 (56.6% complete)   lr: 0.001629   
2019-08-22 13:50:39,699 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 78/135   Jobs: 4   Epoch: 2.29/4.0 (57.4% complete)   lr: 0.001602   
2019-08-22 13:51:33,906 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 79/135   Jobs: 4   Epoch: 2.32/4.0 (58.1% complete)   lr: 0.001575   
2019-08-22 13:52:27,566 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 80/135   Jobs: 4   Epoch: 2.35/4.0 (58.8% complete)   lr: 0.001549   
2019-08-22 13:53:27,621 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 81/135   Jobs: 4   Epoch: 2.38/4.0 (59.6% complete)   lr: 0.001523   
2019-08-22 13:54:21,770 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 82/135   Jobs: 4   Epoch: 2.41/4.0 (60.3% complete)   lr: 0.001497   
2019-08-22 13:55:15,382 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 83/135   Jobs: 4   Epoch: 2.44/4.0 (61.0% complete)   lr: 0.001472   
2019-08-22 13:56:09,136 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 84/135   Jobs: 4   Epoch: 2.47/4.0 (61.8% complete)   lr: 0.001447   
2019-08-22 13:57:02,785 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 85/135   Jobs: 4   Epoch: 2.50/4.0 (62.5% complete)   lr: 0.001423   
2019-08-22 13:57:56,724 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 86/135   Jobs: 4   Epoch: 2.53/4.0 (63.2% complete)   lr: 0.001399   
2019-08-22 13:58:50,427 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 87/135   Jobs: 4   Epoch: 2.56/4.0 (64.0% complete)   lr: 0.001375   
2019-08-22 13:59:43,836 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 88/135   Jobs: 4   Epoch: 2.59/4.0 (64.7% complete)   lr: 0.001352   
2019-08-22 14:00:38,469 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 89/135   Jobs: 4   Epoch: 2.62/4.0 (65.4% complete)   lr: 0.001330   
2019-08-22 14:01:32,495 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 90/135   Jobs: 4   Epoch: 2.65/4.0 (66.2% complete)   lr: 0.001307   
2019-08-22 14:02:26,544 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 91/135   Jobs: 4   Epoch: 2.68/4.0 (66.9% complete)   lr: 0.001285   
2019-08-22 14:03:20,518 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 92/135   Jobs: 4   Epoch: 2.71/4.0 (67.6% complete)   lr: 0.001264   
2019-08-22 14:04:14,690 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 93/135   Jobs: 4   Epoch: 2.74/4.0 (68.4% complete)   lr: 0.001243   
2019-08-22 14:05:08,528 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 94/135   Jobs: 4   Epoch: 2.76/4.0 (69.1% complete)   lr: 0.001222   
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new/configs/network.xconfig --config-dir exp/nnet3/tdnn_new/configs/
nnet3-init exp/nnet3/tdnn_new/configs//init.config exp/nnet3/tdnn_new/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//init.raw
nnet3-info exp/nnet3/tdnn_new/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
2019-08-22 15:48:45,454 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=94 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 2 --trainer.optimization.num-jobs-final 2 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/nnet3/tdnn_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new
['steps/nnet3/train_dnn.py', '--stage=94', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '2', '--trainer.optimization.num-jobs-final', '2', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/nnet3/tdnn_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new']
2019-08-22 15:48:45,477 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/nnet3/tdnn_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 2,
 'num_jobs_initial': 2,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': 94,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-22 15:48:45,893 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 272 iterations
2019-08-22 15:48:45,893 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 94/271   Jobs: 2   Epoch: 1.38/4.0 (34.6% complete)   lr: 0.001354   
2019-08-22 15:49:50,937 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 95/271   Jobs: 2   Epoch: 1.40/4.0 (34.9% complete)   lr: 0.001342   
2019-08-22 15:50:52,141 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 96/271   Jobs: 2   Epoch: 1.41/4.0 (35.3% complete)   lr: 0.001331   
2019-08-22 15:51:53,460 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 97/271   Jobs: 2   Epoch: 1.43/4.0 (35.7% complete)   lr: 0.001320   
2019-08-22 15:52:55,168 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 98/271   Jobs: 2   Epoch: 1.44/4.0 (36.0% complete)   lr: 0.001309   
2019-08-22 15:53:56,813 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 99/271   Jobs: 2   Epoch: 1.46/4.0 (36.4% complete)   lr: 0.001298   
2019-08-22 15:54:58,385 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 100/271   Jobs: 2   Epoch: 1.47/4.0 (36.8% complete)   lr: 0.001287   
2019-08-22 15:56:11,267 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 101/271   Jobs: 2   Epoch: 1.49/4.0 (37.1% complete)   lr: 0.001276   
2019-08-22 15:57:13,177 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 102/271   Jobs: 2   Epoch: 1.50/4.0 (37.5% complete)   lr: 0.001265   
2019-08-22 15:58:14,988 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 103/271   Jobs: 2   Epoch: 1.51/4.0 (37.9% complete)   lr: 0.001254   
2019-08-22 15:59:16,698 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 104/271   Jobs: 2   Epoch: 1.53/4.0 (38.2% complete)   lr: 0.001244   
2019-08-22 16:00:18,401 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 105/271   Jobs: 2   Epoch: 1.54/4.0 (38.6% complete)   lr: 0.001233   
2019-08-22 16:01:20,085 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 106/271   Jobs: 2   Epoch: 1.56/4.0 (39.0% complete)   lr: 0.001223   
2019-08-22 16:02:21,856 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 107/271   Jobs: 2   Epoch: 1.57/4.0 (39.3% complete)   lr: 0.001213   
2019-08-22 16:03:23,403 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 108/271   Jobs: 2   Epoch: 1.59/4.0 (39.7% complete)   lr: 0.001202   
2019-08-22 16:04:25,224 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 109/271   Jobs: 2   Epoch: 1.60/4.0 (40.1% complete)   lr: 0.001192   
2019-08-22 16:05:27,020 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 110/271   Jobs: 2   Epoch: 1.62/4.0 (40.4% complete)   lr: 0.001182   
2019-08-22 16:06:28,790 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 111/271   Jobs: 2   Epoch: 1.63/4.0 (40.8% complete)   lr: 0.001172   
2019-08-22 16:07:30,521 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 112/271   Jobs: 2   Epoch: 1.65/4.0 (41.2% complete)   lr: 0.001162   
2019-08-22 16:08:31,941 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 113/271   Jobs: 2   Epoch: 1.66/4.0 (41.5% complete)   lr: 0.001153   
2019-08-22 16:09:33,902 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 114/271   Jobs: 2   Epoch: 1.68/4.0 (41.9% complete)   lr: 0.001143   
2019-08-22 16:10:35,545 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 115/271   Jobs: 2   Epoch: 1.69/4.0 (42.3% complete)   lr: 0.001133   
2019-08-22 16:11:37,221 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 116/271   Jobs: 2   Epoch: 1.71/4.0 (42.6% complete)   lr: 0.001124   
2019-08-22 16:12:39,094 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 117/271   Jobs: 2   Epoch: 1.72/4.0 (43.0% complete)   lr: 0.001114   
2019-08-22 16:13:40,821 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 118/271   Jobs: 2   Epoch: 1.74/4.0 (43.4% complete)   lr: 0.001105   
2019-08-22 16:14:42,355 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 119/271   Jobs: 2   Epoch: 1.75/4.0 (43.8% complete)   lr: 0.001096   
2019-08-22 16:15:43,935 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 120/271   Jobs: 2   Epoch: 1.76/4.0 (44.1% complete)   lr: 0.001086   
2019-08-22 16:16:56,668 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 121/271   Jobs: 2   Epoch: 1.78/4.0 (44.5% complete)   lr: 0.001077   
2019-08-22 16:17:58,233 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 122/271   Jobs: 2   Epoch: 1.79/4.0 (44.9% complete)   lr: 0.001068   
2019-08-22 16:19:00,233 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 123/271   Jobs: 2   Epoch: 1.81/4.0 (45.2% complete)   lr: 0.001059   
2019-08-22 16:20:01,893 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 124/271   Jobs: 2   Epoch: 1.82/4.0 (45.6% complete)   lr: 0.001050   
2019-08-22 16:21:03,366 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 125/271   Jobs: 2   Epoch: 1.84/4.0 (46.0% complete)   lr: 0.001041   
2019-08-22 16:22:05,355 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 126/271   Jobs: 2   Epoch: 1.85/4.0 (46.3% complete)   lr: 0.001032   
2019-08-22 16:23:12,318 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 127/271   Jobs: 2   Epoch: 1.87/4.0 (46.7% complete)   lr: 0.001024   
2019-08-22 16:24:21,876 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 128/271   Jobs: 2   Epoch: 1.88/4.0 (47.1% complete)   lr: 0.001015   
2019-08-22 16:25:36,300 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 129/271   Jobs: 2   Epoch: 1.90/4.0 (47.4% complete)   lr: 0.001007   
2019-08-22 16:27:26,912 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 130/271   Jobs: 2   Epoch: 1.91/4.0 (47.8% complete)   lr: 0.000998   
2019-08-22 16:29:19,462 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 131/271   Jobs: 2   Epoch: 1.93/4.0 (48.2% complete)   lr: 0.000990   
2019-08-22 16:32:00,921 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 132/271   Jobs: 2   Epoch: 1.94/4.0 (48.5% complete)   lr: 0.000981   
2019-08-22 16:33:49,995 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 133/271   Jobs: 2   Epoch: 1.96/4.0 (48.9% complete)   lr: 0.000973   
2019-08-22 16:35:06,516 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 134/271   Jobs: 2   Epoch: 1.97/4.0 (49.3% complete)   lr: 0.000965   
2019-08-22 16:36:09,903 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 135/271   Jobs: 2   Epoch: 1.99/4.0 (49.6% complete)   lr: 0.000957   
2019-08-22 16:37:12,670 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 136/271   Jobs: 2   Epoch: 2.00/4.0 (50.0% complete)   lr: 0.000949   
2019-08-22 16:38:17,960 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 137/271   Jobs: 2   Epoch: 2.01/4.0 (50.4% complete)   lr: 0.000941   
2019-08-22 16:39:23,373 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 138/271   Jobs: 2   Epoch: 2.03/4.0 (50.7% complete)   lr: 0.000933   
2019-08-22 16:40:28,493 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 139/271   Jobs: 2   Epoch: 2.04/4.0 (51.1% complete)   lr: 0.000925   
2019-08-22 16:41:33,879 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 140/271   Jobs: 2   Epoch: 2.06/4.0 (51.5% complete)   lr: 0.000917   
2019-08-22 16:42:52,887 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 141/271   Jobs: 2   Epoch: 2.07/4.0 (51.8% complete)   lr: 0.000909   
2019-08-22 16:43:57,554 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 142/271   Jobs: 2   Epoch: 2.09/4.0 (52.2% complete)   lr: 0.000902   
2019-08-22 16:45:03,574 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 143/271   Jobs: 2   Epoch: 2.10/4.0 (52.6% complete)   lr: 0.000894   
2019-08-22 16:46:08,475 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 144/271   Jobs: 2   Epoch: 2.12/4.0 (52.9% complete)   lr: 0.000887   
2019-08-22 16:47:12,576 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 145/271   Jobs: 2   Epoch: 2.13/4.0 (53.3% complete)   lr: 0.000879   
2019-08-22 16:48:11,563 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 146/271   Jobs: 2   Epoch: 2.15/4.0 (53.7% complete)   lr: 0.000872   
2019-08-22 16:49:05,087 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 147/271   Jobs: 2   Epoch: 2.16/4.0 (54.0% complete)   lr: 0.000864   
2019-08-22 16:49:57,330 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 148/271   Jobs: 2   Epoch: 2.18/4.0 (54.4% complete)   lr: 0.000857   
2019-08-22 16:50:46,664 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 149/271   Jobs: 2   Epoch: 2.19/4.0 (54.8% complete)   lr: 0.000850   
2019-08-22 16:51:35,662 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 150/271   Jobs: 2   Epoch: 2.21/4.0 (55.1% complete)   lr: 0.000843   
2019-08-22 16:52:24,861 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 151/271   Jobs: 2   Epoch: 2.22/4.0 (55.5% complete)   lr: 0.000836   
2019-08-22 16:53:13,691 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 152/271   Jobs: 2   Epoch: 2.24/4.0 (55.9% complete)   lr: 0.000829   
2019-08-22 16:54:02,849 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 153/271   Jobs: 2   Epoch: 2.25/4.0 (56.2% complete)   lr: 0.000822   
2019-08-22 16:54:51,995 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 154/271   Jobs: 2   Epoch: 2.26/4.0 (56.6% complete)   lr: 0.000815   
2019-08-22 16:55:41,206 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 155/271   Jobs: 2   Epoch: 2.28/4.0 (57.0% complete)   lr: 0.000808   
2019-08-22 16:56:30,127 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 156/271   Jobs: 2   Epoch: 2.29/4.0 (57.4% complete)   lr: 0.000801   
2019-08-22 16:57:19,099 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 157/271   Jobs: 2   Epoch: 2.31/4.0 (57.7% complete)   lr: 0.000794   
2019-08-22 16:58:08,234 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 158/271   Jobs: 2   Epoch: 2.32/4.0 (58.1% complete)   lr: 0.000787   
2019-08-22 16:58:57,165 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 159/271   Jobs: 2   Epoch: 2.34/4.0 (58.5% complete)   lr: 0.000781   
2019-08-22 16:59:46,179 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 160/271   Jobs: 2   Epoch: 2.35/4.0 (58.8% complete)   lr: 0.000774   
2019-08-22 17:00:42,241 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 161/271   Jobs: 2   Epoch: 2.37/4.0 (59.2% complete)   lr: 0.000768   
2019-08-22 17:01:31,269 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 162/271   Jobs: 2   Epoch: 2.38/4.0 (59.6% complete)   lr: 0.000761   
2019-08-22 17:02:20,270 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 163/271   Jobs: 2   Epoch: 2.40/4.0 (59.9% complete)   lr: 0.000755   
2019-08-22 17:03:09,440 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 164/271   Jobs: 2   Epoch: 2.41/4.0 (60.3% complete)   lr: 0.000748   
2019-08-22 17:03:58,687 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 165/271   Jobs: 2   Epoch: 2.43/4.0 (60.7% complete)   lr: 0.000742   
2019-08-22 17:04:47,784 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 166/271   Jobs: 2   Epoch: 2.44/4.0 (61.0% complete)   lr: 0.000736   
2019-08-22 17:05:36,996 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 167/271   Jobs: 2   Epoch: 2.46/4.0 (61.4% complete)   lr: 0.000730   
2019-08-22 17:06:25,766 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 168/271   Jobs: 2   Epoch: 2.47/4.0 (61.8% complete)   lr: 0.000724   
2019-08-22 17:07:14,977 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 169/271   Jobs: 2   Epoch: 2.49/4.0 (62.1% complete)   lr: 0.000717   
2019-08-22 17:08:03,624 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 170/271   Jobs: 2   Epoch: 2.50/4.0 (62.5% complete)   lr: 0.000711   
2019-08-22 17:08:52,341 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 171/271   Jobs: 2   Epoch: 2.51/4.0 (62.9% complete)   lr: 0.000705   
2019-08-22 17:09:40,998 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 172/271   Jobs: 2   Epoch: 2.53/4.0 (63.2% complete)   lr: 0.000699   
2019-08-22 17:10:30,092 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 173/271   Jobs: 2   Epoch: 2.54/4.0 (63.6% complete)   lr: 0.000694   
2019-08-22 17:11:18,935 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 174/271   Jobs: 2   Epoch: 2.56/4.0 (64.0% complete)   lr: 0.000688   
2019-08-22 17:12:07,814 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 175/271   Jobs: 2   Epoch: 2.57/4.0 (64.3% complete)   lr: 0.000682   
2019-08-22 17:12:56,564 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 176/271   Jobs: 2   Epoch: 2.59/4.0 (64.7% complete)   lr: 0.000676   
2019-08-22 17:13:45,441 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 177/271   Jobs: 2   Epoch: 2.60/4.0 (65.1% complete)   lr: 0.000670   
2019-08-22 17:14:33,991 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 178/271   Jobs: 2   Epoch: 2.62/4.0 (65.4% complete)   lr: 0.000665   
2019-08-22 17:15:22,716 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 179/271   Jobs: 2   Epoch: 2.63/4.0 (65.8% complete)   lr: 0.000659   
2019-08-22 17:16:11,601 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 180/271   Jobs: 2   Epoch: 2.65/4.0 (66.2% complete)   lr: 0.000654   
run.sh: line 142:  4544 Killed                  local/nnet3/run_tdnn.sh
this shell script execution duration: 5305 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new/configs/network.xconfig --config-dir exp/nnet3/tdnn_new/configs/
nnet3-init exp/nnet3/tdnn_new/configs//init.config exp/nnet3/tdnn_new/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//init.raw
nnet3-info exp/nnet3/tdnn_new/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
2019-08-23 10:10:37,065 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=180 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 2 --trainer.optimization.num-jobs-final 2 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/nnet3/tdnn_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new
['steps/nnet3/train_dnn.py', '--stage=180', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '2', '--trainer.optimization.num-jobs-final', '2', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/nnet3/tdnn_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new']
2019-08-23 10:10:37,085 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/nnet3/tdnn_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 2,
 'num_jobs_initial': 2,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': 180,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-23 10:10:37,117 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 272 iterations
2019-08-23 10:10:37,118 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 180/271   Jobs: 2   Epoch: 2.65/4.0 (66.2% complete)   lr: 0.000654   
2019-08-23 10:11:46,712 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 181/271   Jobs: 2   Epoch: 2.66/4.0 (66.5% complete)   lr: 0.000648   
run.pl: job failed, log is in exp/nnet3/tdnn_new/log/train.181.2.log
2019-08-23 10:12:46,305 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new/log/train.181.2.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new/cache.181                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.5                     --backstitch-training-interval=1                     --srand=181                       "nnet3-copy --learning-rate=0.00064815679485 --scale=1.0 exp/nnet3/tdnn_new/181.mdl - |" "ark,bg:nnet3-copy-egs --frame=4              ark:exp/nnet3/tdnn_new/egs/egs.7.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=181 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new/182.2.raw
this shell script execution duration: 132 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new/configs/network.xconfig --config-dir exp/nnet3/tdnn_new/configs/
nnet3-init exp/nnet3/tdnn_new/configs//init.config exp/nnet3/tdnn_new/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//init.raw
nnet3-info exp/nnet3/tdnn_new/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new/configs//ref.config exp/nnet3/tdnn_new/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new/configs//ref.raw 
2019-08-23 10:26:36,683 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=181 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 2 --trainer.optimization.num-jobs-final 2 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/nnet3/tdnn_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new
['steps/nnet3/train_dnn.py', '--stage=181', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '2', '--trainer.optimization.num-jobs-final', '2', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/nnet3/tdnn_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new']
2019-08-23 10:26:36,703 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/nnet3/tdnn_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 2,
 'num_jobs_initial': 2,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': 181,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-23 10:26:36,738 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 272 iterations
2019-08-23 10:26:36,739 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 181/271   Jobs: 2   Epoch: 2.66/4.0 (66.5% complete)   lr: 0.000648   
2019-08-23 10:27:39,990 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 182/271   Jobs: 2   Epoch: 2.68/4.0 (66.9% complete)   lr: 0.000643   
2019-08-23 10:28:28,911 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 183/271   Jobs: 2   Epoch: 2.69/4.0 (67.3% complete)   lr: 0.000637   
2019-08-23 10:29:19,634 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 184/271   Jobs: 2   Epoch: 2.71/4.0 (67.6% complete)   lr: 0.000632   
2019-08-23 10:30:10,291 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 185/271   Jobs: 2   Epoch: 2.72/4.0 (68.0% complete)   lr: 0.000627   
2019-08-23 10:31:01,977 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 186/271   Jobs: 2   Epoch: 2.74/4.0 (68.4% complete)   lr: 0.000621   
2019-08-23 10:31:52,751 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 187/271   Jobs: 2   Epoch: 2.75/4.0 (68.8% complete)   lr: 0.000616   
2019-08-23 10:32:45,162 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 188/271   Jobs: 2   Epoch: 2.76/4.0 (69.1% complete)   lr: 0.000611   
2019-08-23 10:33:36,695 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 189/271   Jobs: 2   Epoch: 2.78/4.0 (69.5% complete)   lr: 0.000606   
2019-08-23 10:34:29,150 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 190/271   Jobs: 2   Epoch: 2.79/4.0 (69.9% complete)   lr: 0.000601   
2019-08-23 10:35:19,968 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 191/271   Jobs: 2   Epoch: 2.81/4.0 (70.2% complete)   lr: 0.000596   
2019-08-23 10:36:12,920 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 192/271   Jobs: 2   Epoch: 2.82/4.0 (70.6% complete)   lr: 0.000591   
2019-08-23 10:37:03,499 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 193/271   Jobs: 2   Epoch: 2.84/4.0 (71.0% complete)   lr: 0.000586   
2019-08-23 10:37:57,800 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 194/271   Jobs: 2   Epoch: 2.85/4.0 (71.3% complete)   lr: 0.000581   
2019-08-23 10:38:49,520 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 195/271   Jobs: 2   Epoch: 2.87/4.0 (71.7% complete)   lr: 0.000576   
2019-08-23 10:39:42,911 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 196/271   Jobs: 2   Epoch: 2.88/4.0 (72.1% complete)   lr: 0.000571   
2019-08-23 10:40:34,290 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 197/271   Jobs: 2   Epoch: 2.90/4.0 (72.4% complete)   lr: 0.000566   
2019-08-23 10:41:27,128 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 198/271   Jobs: 2   Epoch: 2.91/4.0 (72.8% complete)   lr: 0.000561   
2019-08-23 10:42:18,240 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 199/271   Jobs: 2   Epoch: 2.93/4.0 (73.2% complete)   lr: 0.000557   
2019-08-23 10:43:10,736 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 200/271   Jobs: 2   Epoch: 2.94/4.0 (73.5% complete)   lr: 0.000552   
2019-08-23 10:44:08,332 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 201/271   Jobs: 2   Epoch: 2.96/4.0 (73.9% complete)   lr: 0.000547   
2019-08-23 10:45:01,125 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 202/271   Jobs: 2   Epoch: 2.97/4.0 (74.3% complete)   lr: 0.000543   
2019-08-23 10:45:51,905 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 203/271   Jobs: 2   Epoch: 2.99/4.0 (74.6% complete)   lr: 0.000538   
2019-08-23 10:46:44,190 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 204/271   Jobs: 2   Epoch: 3.00/4.0 (75.0% complete)   lr: 0.000533   
2019-08-23 10:47:35,946 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 205/271   Jobs: 2   Epoch: 3.01/4.0 (75.4% complete)   lr: 0.000529   
2019-08-23 10:48:28,557 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 206/271   Jobs: 2   Epoch: 3.03/4.0 (75.7% complete)   lr: 0.000525   
2019-08-23 10:49:19,370 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 207/271   Jobs: 2   Epoch: 3.04/4.0 (76.1% complete)   lr: 0.000520   
2019-08-23 10:50:11,628 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 208/271   Jobs: 2   Epoch: 3.06/4.0 (76.5% complete)   lr: 0.000516   
2019-08-23 10:51:02,385 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 209/271   Jobs: 2   Epoch: 3.07/4.0 (76.8% complete)   lr: 0.000511   
2019-08-23 10:51:55,079 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 210/271   Jobs: 2   Epoch: 3.09/4.0 (77.2% complete)   lr: 0.000507   
2019-08-23 10:52:46,191 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 211/271   Jobs: 2   Epoch: 3.10/4.0 (77.6% complete)   lr: 0.000503   
2019-08-23 10:53:38,473 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 212/271   Jobs: 2   Epoch: 3.12/4.0 (77.9% complete)   lr: 0.000499   
2019-08-23 10:54:29,388 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 213/271   Jobs: 2   Epoch: 3.13/4.0 (78.3% complete)   lr: 0.000494   
2019-08-23 10:55:21,730 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 214/271   Jobs: 2   Epoch: 3.15/4.0 (78.7% complete)   lr: 0.000490   
2019-08-23 10:56:12,291 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 215/271   Jobs: 2   Epoch: 3.16/4.0 (79.0% complete)   lr: 0.000486   
2019-08-23 10:57:05,250 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 216/271   Jobs: 2   Epoch: 3.18/4.0 (79.4% complete)   lr: 0.000482   
2019-08-23 10:57:56,527 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 217/271   Jobs: 2   Epoch: 3.19/4.0 (79.8% complete)   lr: 0.000478   
2019-08-23 10:58:51,180 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 218/271   Jobs: 2   Epoch: 3.21/4.0 (80.1% complete)   lr: 0.000474   
2019-08-23 10:59:55,536 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 219/271   Jobs: 2   Epoch: 3.22/4.0 (80.5% complete)   lr: 0.000470   
2019-08-23 11:00:57,614 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 220/271   Jobs: 2   Epoch: 3.24/4.0 (80.9% complete)   lr: 0.000466   
2019-08-23 11:02:12,307 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 221/271   Jobs: 2   Epoch: 3.25/4.0 (81.2% complete)   lr: 0.000462   
run.sh: line 142:  9507 Killed                  local/nnet3/run_tdnn.sh
this shell script execution duration: 2170 s
bash: line 1: 25535 Killed                  ( nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new/cache.221 --print-interval=10 --momentum=0.0 --max-param-change=2.0 --backstitch-training-scale=0.0 --l2-regularize-factor=0.5 --backstitch-training-interval=1 --srand=221 "nnet3-copy --learning-rate=0.000461977957818 --scale=1.0 exp/nnet3/tdnn_new/221.mdl - |" "ark,bg:nnet3-copy-egs --frame=4              ark:exp/nnet3/tdnn_new/egs/egs.2.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=221 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |" exp/nnet3/tdnn_new/222.2.raw ) 2>> exp/nnet3/tdnn_new/log/train.221.2.log >> exp/nnet3/tdnn_new/log/train.221.2.log
bash: line 1: 25536 Killed                  ( nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new/cache.221 --write-cache=exp/nnet3/tdnn_new/cache.222 --print-interval=10 --momentum=0.0 --max-param-change=2.0 --backstitch-training-scale=0.0 --l2-regularize-factor=0.5 --backstitch-training-interval=1 --srand=221 "nnet3-copy --learning-rate=0.000461977957818 --scale=1.0 exp/nnet3/tdnn_new/221.mdl - |" "ark,bg:nnet3-copy-egs --frame=3              ark:exp/nnet3/tdnn_new/egs/egs.1.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=221 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |" exp/nnet3/tdnn_new/222.1.raw ) 2>> exp/nnet3/tdnn_new/log/train.221.1.log >> exp/nnet3/tdnn_new/log/train.221.1.log
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
ERROR:root:***Exception caught while parsing the following xconfig line:
***   relu-renorm-layer name=tdnn1 dim=850

Traceback (most recent call last):
  File "steps/nnet3/xconfig_to_configs.py", line 333, in <module>
    main()
  File "steps/nnet3/xconfig_to_configs.py", line 323, in main
    all_layers = xparser.read_xconfig_file(args.xconfig_file, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 198, in read_xconfig_file
    this_layer = xconfig_line_to_object(line, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 105, in xconfig_line_to_object
    return config_to_layer[first_token](first_token, key_to_value, prev_layers)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 707, in __init__
    XconfigLayerBase.__init__(self, first_token, key_to_value, prev_names)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 60, in __init__
    "layer.".format(self.name))
RuntimeError: Name 'tdnn1' is used for more than one layer.
this shell script execution duration: 0 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tdnn_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
nnet3-init exp/nnet3/tdnn_new_2/configs//init.config exp/nnet3/tdnn_new_2/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//init.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
2019-08-25 13:46:14,003 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=-10 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 2 --trainer.optimization.num-jobs-final 2 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/nnet3/tdnn_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new_2
['steps/nnet3/train_dnn.py', '--stage=-10', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '2', '--trainer.optimization.num-jobs-final', '2', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/nnet3/tdnn_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new_2']
2019-08-25 13:46:14,024 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/nnet3/tdnn_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new_2',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 2,
 'num_jobs_initial': 2,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-25 13:46:17,522 [steps/nnet3/train_dnn.py:230 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-25 13:46:17,571 [steps/nnet3/train_dnn.py:240 - train - INFO ] Generating egs
----------get_egs.sh----------------
steps/nnet3/get_egs.sh --cmd run.pl --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir  --left-context 8 --right-context 8 --left-context-initial -1 --right-context-final -1 --stage 0 --samples-per-iter 400000 --frames-per-eg 8 --srand 0 data/train exp/nnet3/tdnn_ali exp/nnet3/tdnn_new_2/egs
steps/nnet3/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/nnet3/tdnn_new_2/egs/.nodelete
steps/nnet3/get_egs.sh: feature type is raw
steps/nnet3/get_egs.sh: working out number of frames of training data
steps/nnet3/get_egs.sh: working out feature dim
steps/nnet3/get_egs.sh: creating 17 archives, each with 399314 egs, with
steps/nnet3/get_egs.sh:   8 labels per example, and (left,right) context = (8,8)
steps/nnet3/get_egs.sh: copying data alignments
copy-int-vector ark:- ark,scp:exp/nnet3/tdnn_new_2/egs/ali.ark,exp/nnet3/tdnn_new_2/egs/ali.scp 
LOG (copy-int-vector[5.5.458~2-84ab]:main():copy-int-vector.cc:83) Copied 120098 vectors of int32.
steps/nnet3/get_egs.sh: Getting validation and training subset examples.
steps/nnet3/get_egs.sh: ... extracting validation and training-subset alignments.
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/get_egs.sh: Generating training examples on disk
steps/nnet3/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/get_egs.sh: removing temporary archives
steps/nnet3/get_egs.sh: removing temporary alignments
steps/nnet3/get_egs.sh: Finished preparing training examples
2019-08-25 13:47:52,579 [steps/nnet3/train_dnn.py:276 - train - INFO ] Computing the preconditioning matrix for input features
2019-08-25 13:48:01,774 [steps/nnet3/train_dnn.py:287 - train - INFO ] Computing initial vector for FixedScaleComponent before softmax, using priors^-0.25 and rescaling to average 1
2019-08-25 13:48:03,681 [steps/nnet3/train_dnn.py:293 - train - INFO ] Preparing the initial acoustic model.
2019-08-25 13:48:11,306 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 272 iterations
2019-08-25 13:48:11,306 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 0/271   Jobs: 2   Epoch: 0.00/4.0 (0.0% complete)   lr: 0.003000   
2019-08-25 13:48:52,695 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 1/271   Jobs: 2   Epoch: 0.01/4.0 (0.4% complete)   lr: 0.002975   
2019-08-25 13:49:24,863 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 2/271   Jobs: 2   Epoch: 0.03/4.0 (0.7% complete)   lr: 0.002950   
2019-08-25 13:49:57,767 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 3/271   Jobs: 2   Epoch: 0.04/4.0 (1.1% complete)   lr: 0.002925   
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tri5a_ali/tree 
ERROR (tree-info[5.5.458~2-84ab]:Input():kaldi-io.cc:756) Error opening input stream exp/nnet3/tri5a_ali/tree

[ Stack-Trace: ]
tree-info(kaldi::MessageLogger::LogMessage() const+0x8b7) [0x4d3bfd]
tree-info(kaldi::MessageLogger::LogAndThrow::operator=(kaldi::MessageLogger const&)+0x11) [0x437995]
tree-info(kaldi::Input::Input(std::string const&, bool*)+0x8a) [0x46524c]
tree-info(main+0xa7) [0x433044]
/lib64/libc.so.6(__libc_start_main+0xf5) [0x7f4c782f4b35]
tree-info() [0x432ed9]

kaldi::KaldiFatalErrorsteps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
ERROR:root:***Exception caught while parsing the following xconfig line:
***   output-layer name=output input=tdnn6 dim= max-change=1.5

Traceback (most recent call last):
  File "steps/nnet3/xconfig_to_configs.py", line 333, in <module>
    main()
  File "steps/nnet3/xconfig_to_configs.py", line 323, in main
    all_layers = xparser.read_xconfig_file(args.xconfig_file, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 198, in read_xconfig_file
    this_layer = xconfig_line_to_object(line, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 105, in xconfig_line_to_object
    return config_to_layer[first_token](first_token, key_to_value, prev_layers)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 488, in __init__
    XconfigLayerBase.__init__(self, first_token, key_to_value, prev_names)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 69, in __init__
    self.set_configs(key_to_value, all_layers)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 105, in set_configs
    value)
  File "steps/libs/nnet3/xconfig/utils.py", line 162, in convert_value_to_type
    key, string_value))
RuntimeError: Invalid configuration value dim= (expected int)
this shell script execution duration: 0 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/nnet3/tri5a_ali/tree 
ERROR (tree-info[5.5.458~2-84ab]:Input():kaldi-io.cc:756) Error opening input stream exp/nnet3/tri5a_ali/tree

[ Stack-Trace: ]
tree-info(kaldi::MessageLogger::LogMessage() const+0x8b7) [0x4d3bfd]
tree-info(kaldi::MessageLogger::LogAndThrow::operator=(kaldi::MessageLogger const&)+0x11) [0x437995]
tree-info(kaldi::Input::Input(std::string const&, bool*)+0x8a) [0x46524c]
tree-info(main+0xa7) [0x433044]
/lib64/libc.so.6(__libc_start_main+0xf5) [0x7f7c336b6b35]
tree-info() [0x432ed9]

kaldi::KaldiFatalErrorsteps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
ERROR:root:***Exception caught while parsing the following xconfig line:
***   output-layer name=output input=tdnn6 dim= max-change=1.5

Traceback (most recent call last):
  File "steps/nnet3/xconfig_to_configs.py", line 333, in <module>
    main()
  File "steps/nnet3/xconfig_to_configs.py", line 323, in main
    all_layers = xparser.read_xconfig_file(args.xconfig_file, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 198, in read_xconfig_file
    this_layer = xconfig_line_to_object(line, existing_layers)
  File "steps/libs/nnet3/xconfig/parser.py", line 105, in xconfig_line_to_object
    return config_to_layer[first_token](first_token, key_to_value, prev_layers)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 488, in __init__
    XconfigLayerBase.__init__(self, first_token, key_to_value, prev_names)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 69, in __init__
    self.set_configs(key_to_value, all_layers)
  File "steps/libs/nnet3/xconfig/basic_layers.py", line 105, in set_configs
    value)
  File "steps/libs/nnet3/xconfig/utils.py", line 162, in convert_value_to_type
    key, string_value))
RuntimeError: Invalid configuration value dim= (expected int)
this shell script execution duration: 0 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/tri5a_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
nnet3-init exp/nnet3/tdnn_new_2/configs//init.config exp/nnet3/tdnn_new_2/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//init.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
2019-08-25 13:56:37,969 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=-10 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 8 --trainer.optimization.num-jobs-final 8 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/tri5a_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new_2
['steps/nnet3/train_dnn.py', '--stage=-10', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '8', '--trainer.optimization.num-jobs-final', '8', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/tri5a_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new_2']
2019-08-25 13:56:37,989 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/tri5a_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new_2',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 8,
 'num_jobs_initial': 8,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-25 13:56:38,052 [steps/nnet3/train_dnn.py:230 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-25 13:56:38,098 [steps/nnet3/train_dnn.py:240 - train - INFO ] Generating egs
----------get_egs.sh----------------
steps/nnet3/get_egs.sh --cmd run.pl --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir  --left-context 8 --right-context 8 --left-context-initial -1 --right-context-final -1 --stage 0 --samples-per-iter 400000 --frames-per-eg 8 --srand 0 data/train exp/tri5a_ali exp/nnet3/tdnn_new_2/egs
steps/nnet3/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/nnet3/tdnn_new_2/egs/.nodelete
steps/nnet3/get_egs.sh: feature type is raw
steps/nnet3/get_egs.sh: working out number of frames of training data
steps/nnet3/get_egs.sh: working out feature dim
steps/nnet3/get_egs.sh: creating 17 archives, each with 399314 egs, with
steps/nnet3/get_egs.sh:   8 labels per example, and (left,right) context = (8,8)
steps/nnet3/get_egs.sh: copying data alignments
copy-int-vector ark:- ark,scp:exp/nnet3/tdnn_new_2/egs/ali.ark,exp/nnet3/tdnn_new_2/egs/ali.scp 
LOG (copy-int-vector[5.5.458~2-84ab]:main():copy-int-vector.cc:83) Copied 120098 vectors of int32.
steps/nnet3/get_egs.sh: Getting validation and training subset examples.
steps/nnet3/get_egs.sh: ... extracting validation and training-subset alignments.
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/get_egs.sh: Generating training examples on disk
steps/nnet3/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/get_egs.sh: removing temporary archives
steps/nnet3/get_egs.sh: removing temporary alignments
steps/nnet3/get_egs.sh: Finished preparing training examples
2019-08-25 13:57:58,472 [steps/nnet3/train_dnn.py:276 - train - INFO ] Computing the preconditioning matrix for input features
2019-08-25 13:58:06,963 [steps/nnet3/train_dnn.py:287 - train - INFO ] Computing initial vector for FixedScaleComponent before softmax, using priors^-0.25 and rescaling to average 1
2019-08-25 13:58:08,688 [steps/nnet3/train_dnn.py:293 - train - INFO ] Preparing the initial acoustic model.
2019-08-25 13:58:11,351 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 68 iterations
2019-08-25 13:58:11,352 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 0/67   Jobs: 8   Epoch: 0.00/4.0 (0.0% complete)   lr: 0.012000   
2019-08-25 13:58:55,974 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 1/67   Jobs: 8   Epoch: 0.06/4.0 (1.5% complete)   lr: 0.011600   
2019-08-25 13:59:25,062 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 2/67   Jobs: 8   Epoch: 0.12/4.0 (2.9% complete)   lr: 0.011214   
2019-08-25 13:59:53,878 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 3/67   Jobs: 8   Epoch: 0.18/4.0 (4.4% complete)   lr: 0.010841   
2019-08-25 14:00:22,759 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 4/67   Jobs: 8   Epoch: 0.24/4.0 (5.9% complete)   lr: 0.010480   
2019-08-25 14:00:51,862 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 5/67   Jobs: 8   Epoch: 0.29/4.0 (7.4% complete)   lr: 0.010131   
2019-08-25 14:01:20,722 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 6/67   Jobs: 8   Epoch: 0.35/4.0 (8.8% complete)   lr: 0.009794   
2019-08-25 14:01:49,995 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 7/67   Jobs: 8   Epoch: 0.41/4.0 (10.3% complete)   lr: 0.009468   
2019-08-25 14:02:18,992 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 8/67   Jobs: 8   Epoch: 0.47/4.0 (11.8% complete)   lr: 0.009152   
2019-08-25 14:02:47,841 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 9/67   Jobs: 8   Epoch: 0.53/4.0 (13.2% complete)   lr: 0.008848   
2019-08-25 14:03:17,043 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 10/67   Jobs: 8   Epoch: 0.59/4.0 (14.7% complete)   lr: 0.008553   
2019-08-25 14:03:46,177 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 11/67   Jobs: 8   Epoch: 0.65/4.0 (16.2% complete)   lr: 0.008268   
2019-08-25 14:04:22,935 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 12/67   Jobs: 8   Epoch: 0.71/4.0 (17.6% complete)   lr: 0.007993   
2019-08-25 14:04:52,117 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 13/67   Jobs: 8   Epoch: 0.76/4.0 (19.1% complete)   lr: 0.007727   
2019-08-25 14:05:20,916 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 14/67   Jobs: 8   Epoch: 0.82/4.0 (20.6% complete)   lr: 0.007470   
2019-08-25 14:05:50,131 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 15/67   Jobs: 8   Epoch: 0.88/4.0 (22.1% complete)   lr: 0.007221   
2019-08-25 14:06:19,473 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 16/67   Jobs: 8   Epoch: 0.94/4.0 (23.5% complete)   lr: 0.006981   
2019-08-25 14:06:48,746 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 17/67   Jobs: 8   Epoch: 1.00/4.0 (25.0% complete)   lr: 0.006748   
2019-08-25 14:07:19,147 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 18/67   Jobs: 8   Epoch: 1.06/4.0 (26.5% complete)   lr: 0.006523   
2019-08-25 14:07:55,184 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 19/67   Jobs: 8   Epoch: 1.12/4.0 (27.9% complete)   lr: 0.006306   
2019-08-25 14:08:24,220 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 20/67   Jobs: 8   Epoch: 1.18/4.0 (29.4% complete)   lr: 0.006096   
2019-08-25 14:08:57,661 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 21/67   Jobs: 8   Epoch: 1.24/4.0 (30.9% complete)   lr: 0.005893   
2019-08-25 14:09:26,792 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 22/67   Jobs: 8   Epoch: 1.29/4.0 (32.4% complete)   lr: 0.005697   
2019-08-25 14:09:55,720 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 23/67   Jobs: 8   Epoch: 1.35/4.0 (33.8% complete)   lr: 0.005507   
2019-08-25 14:10:31,713 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 24/67   Jobs: 8   Epoch: 1.41/4.0 (35.3% complete)   lr: 0.005324   
2019-08-25 14:10:55,558 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 25/67   Jobs: 8   Epoch: 1.47/4.0 (36.8% complete)   lr: 0.005147   
2019-08-25 14:11:24,772 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 26/67   Jobs: 8   Epoch: 1.53/4.0 (38.2% complete)   lr: 0.004975   
2019-08-25 14:11:53,749 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 27/67   Jobs: 8   Epoch: 1.59/4.0 (39.7% complete)   lr: 0.004810   
2019-08-25 14:12:23,025 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 28/67   Jobs: 8   Epoch: 1.65/4.0 (41.2% complete)   lr: 0.004650   
2019-08-25 14:12:55,678 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 29/67   Jobs: 8   Epoch: 1.71/4.0 (42.6% complete)   lr: 0.004495   
2019-08-25 14:13:21,799 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 30/67   Jobs: 8   Epoch: 1.76/4.0 (44.1% complete)   lr: 0.004345   
2019-08-25 14:13:51,023 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 31/67   Jobs: 8   Epoch: 1.82/4.0 (45.6% complete)   lr: 0.004200   
2019-08-25 14:14:19,930 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 32/67   Jobs: 8   Epoch: 1.88/4.0 (47.1% complete)   lr: 0.004061   
2019-08-25 14:14:48,827 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 33/67   Jobs: 8   Epoch: 1.94/4.0 (48.5% complete)   lr: 0.003925   
2019-08-25 14:15:17,885 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 34/67   Jobs: 8   Epoch: 2.00/4.0 (50.0% complete)   lr: 0.003795   
2019-08-25 14:15:46,964 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 35/67   Jobs: 8   Epoch: 2.06/4.0 (51.5% complete)   lr: 0.003668   
2019-08-25 14:16:15,974 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 36/67   Jobs: 8   Epoch: 2.12/4.0 (52.9% complete)   lr: 0.003546   
2019-08-25 14:16:45,099 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 37/67   Jobs: 8   Epoch: 2.18/4.0 (54.4% complete)   lr: 0.003428   
2019-08-25 14:17:14,099 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 38/67   Jobs: 8   Epoch: 2.24/4.0 (55.9% complete)   lr: 0.003314   
2019-08-25 14:17:43,287 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 39/67   Jobs: 8   Epoch: 2.29/4.0 (57.4% complete)   lr: 0.003204   
2019-08-25 14:18:12,429 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 40/67   Jobs: 8   Epoch: 2.35/4.0 (58.8% complete)   lr: 0.003097   
2019-08-25 14:18:45,389 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 41/67   Jobs: 8   Epoch: 2.41/4.0 (60.3% complete)   lr: 0.002994   
2019-08-25 14:19:14,274 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 42/67   Jobs: 8   Epoch: 2.47/4.0 (61.8% complete)   lr: 0.002894   
2019-08-25 14:19:43,825 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 43/67   Jobs: 8   Epoch: 2.53/4.0 (63.2% complete)   lr: 0.002798   
2019-08-25 14:20:13,064 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 44/67   Jobs: 8   Epoch: 2.59/4.0 (64.7% complete)   lr: 0.002705   
2019-08-25 14:20:41,864 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 45/67   Jobs: 8   Epoch: 2.65/4.0 (66.2% complete)   lr: 0.002615   
2019-08-25 14:21:10,914 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 46/67   Jobs: 8   Epoch: 2.71/4.0 (67.6% complete)   lr: 0.002528   
2019-08-25 14:21:39,978 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 47/67   Jobs: 8   Epoch: 2.76/4.0 (69.1% complete)   lr: 0.002443   
2019-08-25 14:22:09,345 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 48/67   Jobs: 8   Epoch: 2.82/4.0 (70.6% complete)   lr: 0.002362   
2019-08-25 14:22:45,643 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 49/67   Jobs: 8   Epoch: 2.88/4.0 (72.1% complete)   lr: 0.002283   
2019-08-25 14:23:22,376 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 50/67   Jobs: 8   Epoch: 2.94/4.0 (73.5% complete)   lr: 0.002207   
2019-08-25 14:23:59,461 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 51/67   Jobs: 8   Epoch: 3.00/4.0 (75.0% complete)   lr: 0.002134   
2019-08-25 14:24:36,210 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 52/67   Jobs: 8   Epoch: 3.06/4.0 (76.5% complete)   lr: 0.002063   
2019-08-25 14:25:12,824 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 53/67   Jobs: 8   Epoch: 3.12/4.0 (77.9% complete)   lr: 0.001994   
2019-08-25 14:25:49,661 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 54/67   Jobs: 8   Epoch: 3.18/4.0 (79.4% complete)   lr: 0.001928   
2019-08-25 14:26:26,254 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 55/67   Jobs: 8   Epoch: 3.24/4.0 (80.9% complete)   lr: 0.001864   
2019-08-25 14:27:02,906 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 56/67   Jobs: 8   Epoch: 3.29/4.0 (82.4% complete)   lr: 0.001802   
2019-08-25 14:27:39,564 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 57/67   Jobs: 8   Epoch: 3.35/4.0 (83.8% complete)   lr: 0.001742   
2019-08-25 14:28:16,181 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 58/67   Jobs: 8   Epoch: 3.41/4.0 (85.3% complete)   lr: 0.001684   
2019-08-25 14:28:52,768 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 59/67   Jobs: 8   Epoch: 3.47/4.0 (86.8% complete)   lr: 0.001628   
2019-08-25 14:29:29,411 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 60/67   Jobs: 8   Epoch: 3.53/4.0 (88.2% complete)   lr: 0.001573   
2019-08-25 14:30:09,758 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 61/67   Jobs: 8   Epoch: 3.59/4.0 (89.7% complete)   lr: 0.001521   
2019-08-25 14:30:46,324 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 62/67   Jobs: 8   Epoch: 3.65/4.0 (91.2% complete)   lr: 0.001470   
2019-08-25 14:31:22,775 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 63/67   Jobs: 8   Epoch: 3.71/4.0 (92.6% complete)   lr: 0.001421   
2019-08-25 14:31:58,713 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 64/67   Jobs: 8   Epoch: 3.76/4.0 (94.1% complete)   lr: 0.001374   
2019-08-25 14:32:35,375 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 65/67   Jobs: 8   Epoch: 3.82/4.0 (95.6% complete)   lr: 0.001328   
2019-08-25 14:33:12,294 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 66/67   Jobs: 8   Epoch: 3.88/4.0 (97.1% complete)   lr: 0.001284   
2019-08-25 14:33:48,896 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 67/67   Jobs: 8   Epoch: 3.94/4.0 (98.5% complete)   lr: 0.001200   
2019-08-25 14:34:25,479 [steps/nnet3/train_dnn.py:399 - train - INFO ] Doing final combination to produce final.mdl
2019-08-25 14:34:25,479 [steps/libs/nnet3/train/frame_level_objf/common.py:491 - combine_models - INFO ] Combining set([64, 65, 66, 67, 68, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]) models.
exp/nnet3/tdnn_new_2: num-iters=68 nj=8..8 num-params=7.7M dim=16->3048 combine=-1.01->-1.01 (over 2) loglike:train/valid[44,67]=(-1.06,-1.01/-1.20,-1.18) accuracy:train/valid[44,67]=(0.68,0.69/0.65,0.65)
cat: data//utt2spk: No such file or directory
local/nnet3/run_tdnn.sh: line 125: 
if [ 0 -le 9 ]; then
  # this version of the decoding treats each utterance separately
  # without carrying forward speaker information.
  for decode_set in dev test; do
    num_jobs=0	# 说话人个数
    decode_dir=exp/nnet3/tdnn_new_2/decode_
    steps/nnet3/decode.sh --nj  --cmd run.pl        exp/tri5a/graph data/  || exit 1;
       #--online-ivector-dir exp/nnet3/ivectors_        #exp/tri5a/graph data/  || exit 1;
  done
fi
: No such file or directory
this shell script execution duration: 2293 s
-------------------------------------
local/nnet3/run_tdnn.sh: creating neural net configs
tree-info exp/tri5a_ali/tree 
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_new_2/configs/network.xconfig --config-dir exp/nnet3/tdnn_new_2/configs/
nnet3-init exp/nnet3/tdnn_new_2/configs//init.config exp/nnet3/tdnn_new_2/configs//init.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//init.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//init.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
nnet3-init exp/nnet3/tdnn_new_2/configs//ref.config exp/nnet3/tdnn_new_2/configs//ref.raw 
LOG (nnet3-init[5.5.458~2-84ab]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/nnet3/tdnn_new_2/configs//ref.raw
nnet3-info exp/nnet3/tdnn_new_2/configs//ref.raw 
2019-08-25 14:37:51,505 [steps/nnet3/train_dnn.py:37 - <module> - INFO ] Starting DNN trainer (train_dnn.py)
steps/nnet3/train_dnn.py --stage=9 --cmd=run.pl --feat.cmvn-opts=--norm-means=false --norm-vars=false --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 8 --trainer.optimization.num-jobs-final 8 --trainer.optimization.initial-effective-lrate 0.0015 --trainer.optimization.final-effective-lrate 0.00015 --egs.dir  --cleanup.remove-egs true --cleanup.preserve-model-interval 500 --use-gpu yes --feat-dir=data/train --ali-dir exp/tri5a_ali --lang data/lang --reporting.email= --trainer.optimization.minibatch-size 256 --dir=exp/nnet3/tdnn_new_2
['steps/nnet3/train_dnn.py', '--stage=9', '--cmd=run.pl', '--feat.cmvn-opts=--norm-means=false --norm-vars=false', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '8', '--trainer.optimization.num-jobs-final', '8', '--trainer.optimization.initial-effective-lrate', '0.0015', '--trainer.optimization.final-effective-lrate', '0.00015', '--egs.dir', '', '--cleanup.remove-egs', 'true', '--cleanup.preserve-model-interval', '500', '--use-gpu', 'yes', '--feat-dir=data/train', '--ali-dir', 'exp/tri5a_ali', '--lang', 'data/lang', '--reporting.email=', '--trainer.optimization.minibatch-size', '256', '--dir=exp/nnet3/tdnn_new_2']
2019-08-25 14:37:51,525 [steps/nnet3/train_dnn.py:179 - train - INFO ] Arguments for the experiment
{'ali_dir': 'exp/tri5a_ali',
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'run.pl',
 'compute_per_dim_accuracy': False,
 'dir': 'exp/nnet3/tdnn_new_2',
 'do_final_combination': True,
 'dropout_schedule': None,
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': None,
 'egs_stage': 0,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train',
 'final_effective_lrate': 0.00015,
 'frames_per_eg': 8,
 'initial_effective_lrate': 0.0015,
 'input_model': None,
 'lang': 'data/lang',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'minibatch_size': '256',
 'momentum': 0.0,
 'num_epochs': 4.0,
 'num_jobs_compute_prior': 10,
 'num_jobs_final': 8,
 'num_jobs_initial': 8,
 'num_jobs_step': 1,
 'online_ivector_dir': None,
 'preserve_model_interval': 500,
 'presoftmax_prior_scale_power': -0.25,
 'prior_subset_size': 20000,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'samples_per_iter': 400000,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': 9,
 'train_opts': [],
 'use_gpu': 'yes'}
2019-08-25 14:37:51,559 [steps/nnet3/train_dnn.py:317 - train - INFO ] Training will run for 4.0 epochs = 68 iterations
2019-08-25 14:37:51,559 [steps/nnet3/train_dnn.py:353 - train - INFO ] Iter: 9/67   Jobs: 8   Epoch: 0.53/4.0 (13.2% complete)   lr: 0.008848   
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/compute_prob_valid.9.log
2019-08-25 14:37:51,609 [steps/libs/common.py:241 - background_command_waiter - WARNING ] Command exited with status 1:  run.pl exp/nnet3/tdnn_new_2/log/compute_prob_valid.9.log                 nnet3-compute-prob "exp/nnet3/tdnn_new_2/9.mdl"                 "ark,bg:nnet3-copy-egs                      ark:exp/nnet3/tdnn_new_2/egs/valid_diagnostic.egs ark:- |                     nnet3-merge-egs --minibatch-size=1:64 ark:-                     ark:- |" 
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/compute_prob_train.9.log
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/progress.9.log
2019-08-25 14:37:51,610 [steps/libs/common.py:241 - background_command_waiter - WARNING ] Command exited with status 1: run.pl exp/nnet3/tdnn_new_2/log/compute_prob_train.9.log                 nnet3-compute-prob  "exp/nnet3/tdnn_new_2/9.mdl"                 "ark,bg:nnet3-copy-egs                      ark:exp/nnet3/tdnn_new_2/egs/train_diagnostic.egs ark:- |                     nnet3-merge-egs --minibatch-size=1:64 ark:-                     ark:- |" 
2019-08-25 14:37:51,611 [steps/libs/common.py:241 - background_command_waiter - WARNING ] Command exited with status 1: run.pl exp/nnet3/tdnn_new_2/log/progress.9.log                     nnet3-info exp/nnet3/tdnn_new_2/9.mdl '&&'                     nnet3-show-progress --use-gpu=no exp/nnet3/tdnn_new_2/8.mdl exp/nnet3/tdnn_new_2/9.mdl 
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.8.log
2019-08-25 14:38:09,159 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.8.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=0              ark:exp/nnet3/tdnn_new_2/egs/egs.12.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.8.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.6.log
2019-08-25 14:38:10,337 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.6.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=6              ark:exp/nnet3/tdnn_new_2/egs/egs.10.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.6.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.3.log
2019-08-25 14:38:10,342 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.3.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=3              ark:exp/nnet3/tdnn_new_2/egs/egs.7.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.3.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.4.log
2019-08-25 14:38:10,347 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.4.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=4              ark:exp/nnet3/tdnn_new_2/egs/egs.8.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.4.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.2.log
2019-08-25 14:38:10,348 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.2.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=2              ark:exp/nnet3/tdnn_new_2/egs/egs.6.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.2.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.7.log
2019-08-25 14:38:10,349 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.7.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=7              ark:exp/nnet3/tdnn_new_2/egs/egs.11.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.7.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.5.log
2019-08-25 14:38:10,351 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.5.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=5              ark:exp/nnet3/tdnn_new_2/egs/egs.9.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.5.raw
run.pl: job failed, log is in exp/nnet3/tdnn_new_2/log/train.9.1.log
2019-08-25 14:38:11,101 [steps/libs/common.py:236 - background_command_waiter - ERROR ] Command exited with status 1: run.pl --gpu 1 exp/nnet3/tdnn_new_2/log/train.9.1.log                     nnet3-train --use-gpu=yes --read-cache=exp/nnet3/tdnn_new_2/cache.9 --write-cache=exp/nnet3/tdnn_new_2/cache.10                       --print-interval=10                     --momentum=0.0                     --max-param-change=2.0                     --backstitch-training-scale=0.0                     --l2-regularize-factor=0.125                     --backstitch-training-interval=1                     --srand=9                       "nnet3-copy --learning-rate=0.00884765755046 --scale=1.0 exp/nnet3/tdnn_new_2/9.mdl - |" "ark,bg:nnet3-copy-egs --frame=1              ark:exp/nnet3/tdnn_new_2/egs/egs.5.ark ark:- |             nnet3-shuffle-egs --buffer-size=5000             --srand=9 ark:- ark:- |              nnet3-merge-egs --minibatch-size=256 ark:- ark:- |"                     exp/nnet3/tdnn_new_2/10.1.raw
this shell script execution duration: 21 s
-------------------------------------
steps/nnet3/decode.sh --nj 40 --cmd run.pl exp/tri5a/graph data/dev exp/nnet3/tdnn_new_2/decode_dev
steps/nnet3/decode.sh: feature type is raw
steps/diagnostic/analyze_lats.sh --cmd run.pl --iter final exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_dev
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn_new_2/decode_dev/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,4,33) and mean=13.9
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn_new_2/decode_dev/log/analyze_lattice_depth_stats.log
score best paths
+ steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_dev
steps/score_kaldi.sh --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_dev
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_dev
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/dev exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_dev
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
score confidence and timing with sclite
Decoding done.
steps/nnet3/decode.sh --nj 20 --cmd run.pl exp/tri5a/graph data/test exp/nnet3/tdnn_new_2/decode_test
steps/nnet3/decode.sh: feature type is raw
steps/diagnostic/analyze_lats.sh --cmd run.pl --iter final exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_test
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn_new_2/decode_test/log/analyze_alignments.log
Overall, lattice depth (10,50,90-percentile)=(1,5,47) and mean=19.8
steps/diagnostic/analyze_lats.sh: see stats in exp/nnet3/tdnn_new_2/decode_test/log/analyze_lattice_depth_stats.log
score best paths
+ steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_test
steps/score_kaldi.sh --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_test
steps/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_test
steps/scoring/score_kaldi_cer.sh --stage 2 --cmd run.pl data/test exp/tri5a/graph exp/nnet3/tdnn_new_2/decode_test
steps/scoring/score_kaldi_cer.sh: scoring with word insertion penalty=0.0,0.5,1.0
+ echo 'local/score.sh: Done'
local/score.sh: Done
score confidence and timing with sclite
Decoding done.
this shell script execution duration: 2365 s
